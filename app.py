# app.py
# CORTEX -- Corpus Explorer Version Alpha (10-Dec-25)

import streamlit as st
import pandas as pd
import numpy as np
import math
from collections import Counter
from io import BytesIO, StringIOÂ 
import tempfileÂ 
import osÂ 
import reÂ 
import requests
import matplotlib.pyplot as pltÂ 

# --- Re-enabled Imports ---
try:
Â  Â  from wordcloud import WordCloudÂ 
Â  Â  WORDCLOUD_FEATURE_AVAILABLE = True
except ImportError:
Â  Â  WORDCLOUD_FEATURE_AVAILABLE = False
Â  Â Â 
try:
Â  Â  from pyvis.network import Network
Â  Â  PYVIS_FEATURE_AVAILABLE = True
except ImportError:
Â  Â  PYVIS_FEATURE_AVAILABLE = False
# --------------------------
Â  Â Â 
try:
Â  Â  from cefrpy import CEFRAnalyzer
Â  Â  CEFR_ANALYZER = CEFRAnalyzer()
Â  Â  CEFR_FEATURE_AVAILABLE = True
except ImportError:
Â  Â  CEFR_FEATURE_AVAILABLE = False
Â  Â Â 
try:
Â  Â  import eng_to_ipa as ipa
Â  Â  IPA_FEATURE_AVAILABLE = True
except ImportError:
Â  Â  IPA_FEATURE_AVAILABLE = False


import streamlit.components.v1 as componentsÂ 
import xml.etree.ElementTree as ET # Import for XML parsing


# We explicitly exclude external LLM libraries for the free, stable version.
# The interpret_results_llm function is replaced with a placeholder.

st.set_page_config(page_title="CORTEX -- Corpus Explorer Version Alpha (10-Dec-25) by PRIHANTORO (www.prihantoro.com; prihantoro@live.undip.ac.id)", layout="wide")Â 

# --- CONSTANTS ---
KWIC_MAX_DISPLAY_LINES = 100
KWIC_INITIAL_DISPLAY_HEIGHT = 10Â 
KWIC_COLLOC_DISPLAY_LIMIT = 20 # Limit for KWIC examples below collocation tables

# Define global names for parallel mode
SOURCE_LANG_CODE = 'EN' # Default source language code
TARGET_LANG_CODE = 'ID' # Default target language code
DEFAULT_LANG_CODE = 'RAW'

# ---------------------------
# Initializing Session State
# ---------------------------
if 'view' not in st.session_state:
Â  Â  st.session_state['view'] = 'overview'
if 'trigger_analyze' not in st.session_state:
Â  Â  st.session_state['trigger_analyze'] = False
if 'initial_load_complete' not in st.session_state:
Â  Â  st.session_state['initial_load_complete'] = False
if 'collocate_pos_regex' not in st.session_state:Â 
Â  Â  st.session_state['collocate_pos_regex'] = ''
if 'pattern_collocate_pos' not in st.session_state:Â 
Â  Â  st.session_state['pattern_collocate_pos'] = ''
if 'collocate_lemma' not in st.session_state:Â 
Â  Â  st.session_state['collocate_lemma'] = ''
if 'llm_interpretation_result' not in st.session_state:
Â  Â  st.session_state['llm_interpretation_result'] = None
# --- Input State (must be initialized for keyed widgets) ---
if 'dict_word_input_main' not in st.session_state:Â 
Â  Â  st.session_state['dict_word_input_main'] = ''
if 'collocate_regex_input' not in st.session_state:Â 
Â  Â  st.session_state['collocate_regex_input'] = ''
if 'pattern_collocate_input' not in st.session_state:
Â  Â  st.session_state['pattern_collocate_input'] = ''
if 'pattern_collocate_pos_input' not in st.session_state:
Â  Â  Â st.session_state['pattern_collocate_pos_input'] = ''
if 'typed_target_input' not in st.session_state:
Â  Â  Â st.session_state['typed_target_input'] = ''
if 'max_collocates' not in st.session_state:
Â  Â  st.session_state['max_collocates'] = 20
if 'coll_window' not in st.session_state:
Â  Â  st.session_state['coll_window'] = 5
if 'mi_min_freq' not in st.session_state:
Â  Â  st.session_state['mi_min_freq'] = 1
# --- N-Gram State ---
if 'n_gram_size' not in st.session_state:
Â  Â  st.session_state['n_gram_size'] = 2
if 'n_gram_filters' not in st.session_state:
Â  Â  st.session_state['n_gram_filters'] = {} # Dictionary to hold positional filters: {'1': 'pattern', '2': 'pattern', ...}
if 'n_gram_trigger_analyze' not in st.session_state:
Â  Â  st.session_state['n_gram_trigger_analyze'] = False
if 'n_gram_results_df' not in st.session_state:
Â  Â  st.session_state['n_gram_results_df'] = pd.DataFrame()
# --- Parallel Corpus State ---
if 'parallel_mode' not in st.session_state:
Â  Â  st.session_state['parallel_mode'] = False
if 'df_target_lang' not in st.session_state:
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
if 'target_sent_map' not in st.session_state:
Â  Â  st.session_state['target_sent_map'] = {}
# --- Monolingual XML state ---
if 'monolingual_xml_file_upload' not in st.session_state:
Â  Â  st.session_state['monolingual_xml_file_upload'] = None
# --- XML Structure Cache ---
if 'xml_structure_data' not in st.session_state:
Â  Â  Â st.session_state['xml_structure_data'] = None
if 'xml_structure_error' not in st.session_state: # NEW: To store XML parsing errors
Â  Â  st.session_state['xml_structure_error'] = None
# --- Display Settings ---
if 'show_pos_tag' not in st.session_state:
Â  Â  st.session_state['show_pos_tag'] = False
if 'show_lemma' not in st.session_state:
Â  Â  st.session_state['show_lemma'] = False
# --- New Language State ---
if 'user_explicit_lang_code' not in st.session_state:
Â  Â  Â st.session_state['user_explicit_lang_code'] = 'EN' # Default to English


# ---------------------------
# Built-in Corpus Configuration (UPDATED)
# ---------------------------
BUILT_IN_CORPORA = {
Â  Â  "Select built-in corpus...": None,
Â  Â  "BROWN (EN XML Tagged)": "https://raw.githubusercontent.com/prihantoro-corpus/corpus-query-systems/main/BrownCorpus.xml",
Â  Â  "KOSLAT (ID XML Tagged)": "https://raw.githubusercontent.com/prihantoro-corpus/corpus-query-systems/main/KOSLAT-full.xml",
}

# Define color map constants globally (used for both graph and word cloud)
POS_COLOR_MAP = {
Â  Â  'N': '#33CC33',Â  # Noun (Green)
Â  Â  'V': '#3366FF',Â  # Verb (Blue)
Â  Â  'J': '#FF33B5',Â  # Adjective (Pink)
Â  Â  'R': '#FFCC00',Â  # Adverb (Yellow)
Â  Â  '#': '#AAAAAA',Â  # Nonsense Tag / Raw (Gray)
Â  Â  'O': '#AAAAAA'Â  Â # Other (Gray)
}

PUNCTUATION = {'.', ',', '!', '?', ';', ':', '(', ')', '[', ']', '{', '}', '"', "'", '---', '--', '-', '...', 'Â«', 'Â»', 'â€”'}

# --------------------------------------------------------
# NEW FUNCTIONS: Zipf and Band Calculation (from v17.45)
# --------------------------------------------------------
def pmw_to_zipf(pmw):
Â  Â  """
Â  Â  Convert frequency per million (PMW) to Zipf scale.
Â  Â  Formula: Zipf = log10(PMW) + 3
Â  Â  """
Â  Â  if pmw <= 0:
Â  Â  Â  Â  return np.nan
Â  Â  return math.log10(pmw) + 3


def zipf_to_band(zipf):
Â  Â  """
Â  Â  Assign 1â€“5 Zipf band based on score:
Â  Â  Band 1: 7.0â€“7.9
Â  Â  Band 2: 6.0â€“6.9
Â  Â  Band 3: 5.0â€“5.9
Â  Â  Band 4: 4.0â€“4.9
Â  Â  Band 5: 1.0â€“3.9
Â  Â  """
Â  Â  if pd.isna(zipf):
Â  Â  Â  Â  return np.nan
Â  Â  elif zipf >= 7.0:
Â  Â  Â  Â  return 1
Â  Â  elif zipf >= 6.0:
Â  Â  Â  Â  return 2
Â  Â  elif zipf >= 5.0:
Â  Â  Â  Â  return 3
Â  Â  elif zipf >= 4.0:
Â  Â  Â  Â  return 4
Â  Â  else:Â 
Â  Â  Â  Â  return 5
# --------------------------------------------------------


# --- Word Cloud Function ---
@st.cache_data
def create_word_cloud(freq_data, is_tagged_mode):
Â  Â  """Generates a word cloud from frequency data with conditional POS coloring."""
Â  Â Â 
Â  Â  # Check added for robustness against sandbox environment
Â  Â  if not WORDCLOUD_FEATURE_AVAILABLE:
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  # Filter out multi-word units for visualization stability
Â  Â  single_word_freq_data = freq_data[~freq_data['token'].str.contains(' ')].copy()
Â  Â  if single_word_freq_data.empty:
Â  Â  Â  Â  return None # SAFE EXIT 1

Â  Â  word_freq_dict = single_word_freq_data.set_index('token')['frequency'].to_dict()
Â  Â  word_to_pos = single_word_freq_data.set_index('token').get('pos', pd.Series('O')).to_dict()
Â  Â Â 
Â  Â  # We must import WordCloud here to use it from within the function
Â  Â  from wordcloud import WordCloudÂ 
Â  Â Â 
Â  Â  # Simple list of English stopwords; can be expanded.
Â  Â  stopwords = set(["the", "of", "to", "and", "in", "that", "is", "a", "for", "on", "it", "with", "as", "by", "this", "be", "are", "have", "not", "will", "i", "we", "you"])
Â  Â Â 
Â  Â  wc = WordCloud(
Â  Â  Â  Â  width=800,
Â  Â  Â  Â  height=400,
Â  Â  Â  Â  background_color='black',
Â  Â  Â  Â  colormap='viridis',Â 
Â  Â  Â  Â  stopwords=stopwords,
Â  Â  Â  Â  min_font_size=10
Â  Â  )
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  wordcloud = wc.generate_from_frequencies(word_freq_dict)
Â  Â  except ValueError:
Â  Â  Â  Â  return None # SAFE EXIT 2: If the dictionary is empty after stopwords filtering

Â  Â  if is_tagged_mode:
Â  Â  Â  Â  def final_color_func(word, *args, **kwargs):
Â  Â  Â  Â  Â  Â  pos_tag = word_to_pos.get(word, 'O')
Â  Â  Â  Â  Â  Â  pos_code = pos_tag[0].upper() if pos_tag and len(pos_tag) > 0 else 'O'
Â  Â  Â  Â  Â  Â  if pos_code not in POS_COLOR_MAP:
Â  Â  Â  Â  Â  Â  Â  Â  pos_code = 'O'
Â  Â  Â  Â  Â  Â  return POS_COLOR_MAP.get(pos_code, POS_COLOR_MAP['O'])

Â  Â  Â  Â  wordcloud = wordcloud.recolor(color_func=final_color_func)
Â  Â  Â  Â Â 
Â  Â  fig, ax = plt.subplots(figsize=(8, 4))
Â  Â  ax.imshow(wordcloud, interpolation='bilinear')
Â  Â  ax.axis("off")
Â  Â  plt.tight_layout(pad=0)
Â  Â Â 
Â  Â  return fig

# --- NAVIGATION FUNCTIONS ---
def set_view(view_name):
Â  Â  st.session_state['view'] = view_name
Â  Â  st.session_state['llm_interpretation_result'] = None
Â  Â Â 
def reset_analysis():
Â  Â  # Clear all cached functions related to old data.
Â  Â  st.cache_data.clear()
Â  Â Â 
Â  Â  # Reset view and flags
Â  Â  st.session_state['view'] = 'overview'
Â  Â  st.session_state['trigger_analyze'] = False
Â  Â  st.session_state['n_gram_trigger_analyze'] = TrueÂ 
Â  Â  st.session_state['n_gram_results_df'] = pd.DataFrame()
Â  Â  st.session_state['initial_load_complete'] = False
Â  Â  st.session_state['llm_interpretation_result'] = None
Â  Â  # Reset parallel corpus state
Â  Â  st.session_state['parallel_mode'] = False
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
Â  Â  st.session_state['target_sent_map'] = {}
Â  Â  st.session_state['monolingual_xml_file_upload'] = None
Â  Â  st.session_state['xml_structure_data'] = None # Clear structure data
Â  Â  st.session_state['xml_structure_error'] = None # Clear structure error
Â  Â Â 
Â  Â  # --- Force a complete script rerun directly inside the callback ---
Â  Â  st.rerun()
Â  Â Â 
# --- Analysis Trigger Callback (for implicit Enter/change) ---
def trigger_analysis_callback():
Â  Â  st.session_state['trigger_analyze'] = True
Â  Â  st.session_state['llm_interpretation_result'] = None

# --- N-Gram Analysis Trigger Callback ---
def trigger_n_gram_analysis_callback():
Â  Â  st.session_state['n_gram_trigger_analyze'] = True

# --- Dictionary Helper: Get all forms by lemma ---
@st.cache_data
def get_all_lemma_forms_details(df_corpus, target_word):
Â  Â  """
Â  Â  Finds all unique tokens/POS pairs sharing the target word's lemma(s).
Â  Â  FIX: All token/lemma output is converted to lowercase.
Â  Â  """
Â  Â  target_lower = target_word.lower()
Â  Â  matching_rows = df_corpus[df_corpus['token'].str.lower() == target_lower]
Â  Â Â 
Â  Â  if matching_rows.empty or 'lemma' not in df_corpus.columns:
Â  Â  Â  Â  return pd.DataFrame(), [], []
Â  Â  Â  Â Â 
Â  Â  unique_lemmas = matching_rows['lemma'].unique()
Â  Â Â 
Â  Â  # Filter out nonsense tags
Â  Â  valid_lemmas = [l for l in unique_lemmas if l not in ('##', '###')]
Â  Â  if not valid_lemmas:
Â  Â  Â  Â  return pd.DataFrame(), [], []

Â  Â  # Get all forms sharing these valid lemmas
Â  Â  all_forms_df = df_corpus[df_corpus['lemma'].isin(valid_lemmas)][['token', 'pos', 'lemma']].copy()
Â  Â Â 
Â  Â  # FIX 1: Convert token and lemma columns to lowercase before dropping duplicates and sorting
Â  Â  all_forms_df['token'] = all_forms_df['token'].str.lower()
Â  Â  all_forms_df['lemma'] = all_forms_df['lemma'].str.lower()
Â  Â Â 
Â  Â  # Keep only unique token-pos-lemma combinations, sorted by token name
Â  Â  forms_list = all_forms_df.drop_duplicates().sort_values('token').reset_index(drop=True)
Â  Â Â 
Â  Â  # Also return the unique POS and Lemma lists for the summary header (re-using old logic)
Â  Â  return forms_list, all_forms_df['pos'].unique(), valid_lemmas

# --- Regex Forms Helper (Caching Removed for Bug Fix) ---
def get_related_forms_by_regex(df_corpus, target_word):
Â  Â  # Construct a broad regex for related forms: .*<target_word>.* (case insensitive)
Â  Â  pattern_str = f".*{re.escape(target_word)}.*"
Â  Â  pattern = re.compile(pattern_str, re.IGNORECASE)
Â  Â Â 
Â  Â  all_unique_tokens = df_corpus['token'].unique()
Â  Â Â 
Â  Â  related_forms = []
Â  Â  for token in all_unique_tokens:
Â  Â  Â  Â  if pattern.fullmatch(token):
Â  Â  Â  Â  Â  Â  related_forms.append(token)
Â  Â  Â  Â  Â  Â Â 
Â  Â  target_lower = target_word.lower()
Â  Â  final_forms = [w for w in related_forms if w.lower() != target_lower]
Â  Â Â 
Â  Â  return sorted(list(set(final_forms)))

# --- LLM PLACEHOLDER ---
def interpret_results_llm(target_word, analysis_type, data_description, data):
Â  Â  """Placeholder for LLM function."""
Â  Â  mock_response = f"""
Â  Â  ### ðŸ§  Feature Currently Unavailable

Â  Â  The external LLM interpretation feature is temporarily disabled due to stability and congestion issues with free public APIs (Gemini/Hugging Face).

Â  Â  **CORTEX recommends focusing on the raw linguistic data** provided in the Concordance, Collocation, and Dictionary modules to draw your own expert conclusion.

Â  Â  **Analysis Context:**
Â  Â  * Target: **"{target_word}"**
Â  Â  * Analysis Type: **{analysis_type}**
Â  Â  """
Â  Â  st.session_state['llm_interpretation_result'] = mock_response
Â  Â  st.warning("LLM Feature Disabled. See 'LLM Interpretation' expander for details.")
Â  Â  return mock_response
Â  Â Â 
# --- KWIC/Concordance Helper Function (Reusable by Dictionary) ---
@st.cache_data(show_spinner=False)
def generate_kwic(df_corpus, raw_target_input, kwic_left, kwic_right, pattern_collocate_input="", pattern_collocate_pos_input="", pattern_window=0, limit=KWIC_MAX_DISPLAY_LINES, random_sample=False, is_parallel_mode=False, show_pos=False, show_lemma=False):
Â  Â  """
Â  Â  Generalized function to generate KWIC lines based on target and optional collocate filter.
Â  Â  Returns: (list_of_kwic_rows, total_matches, primary_target_mwu, literal_freq, list_of_sent_ids, breakdown_df)
Â  Â  """
Â  Â  total_tokens = len(df_corpus)
Â  Â  tokens_lower = df_corpus["_token_low"].tolist()
Â  Â Â 
Â  Â  # --- MWU/WILDCARD/STRUCTURAL RESOLUTION (Unified Search) ---
Â  Â  search_terms = raw_target_input.split()
Â  Â  primary_target_len = len(search_terms)
Â  Â  is_raw_mode = 'pos' not in df_corpus.columns or df_corpus['pos'].str.contains('##', na=False).sum() > 0.99 * len(df_corpus)
Â  Â  is_structural_search = not is_raw_mode and any('[' in t or '_' in t for t in search_terms)
Â  Â Â 
Â  Â  def create_structural_matcher(term):
Â  Â  Â  Â  lemma_pattern = None
Â  Â  Â  Â  pos_pattern = None
Â  Â  Â  Â  lemma_match = re.search(r"\[(.*?)\]", term)
Â  Â  Â  Â  if lemma_match:
Â  Â  Â  Â  Â  Â  lemma_input = lemma_match.group(1).strip().lower()
Â  Â  Â  Â  Â  Â  if lemma_input:
Â  Â  Â  Â  Â  Â  Â  Â  lemma_pattern_str = re.escape(lemma_input).replace(r'\*', '.*')
Â  Â  Â  Â  Â  Â  Â  Â  lemma_pattern = re.compile(f"^{lemma_pattern_str}$")
Â  Â  Â  Â  pos_match = re.search(r"\_([\w\*|]+)", term)
Â  Â  Â  Â  if pos_match:
Â  Â  Â  Â  Â  Â  pos_input = pos_match.group(1).strip()
Â  Â  Â  Â  Â  Â  if pos_input:
Â  Â  Â  Â  Â  Â  Â  Â  pos_patterns = [p.strip() for p in pos_input.split('|') if p.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  full_pos_regex_list = [re.escape(p).replace(r'\*', '.*') for p in pos_patterns]
Â  Â  Â  Â  Â  Â  Â  Â  pos_pattern = re.compile("^(" + "|".join(full_pos_regex_list) + ")$")
Â  Â  Â  Â  if lemma_pattern or pos_pattern:
Â  Â  Â  Â  Â  Â  Â return {'type': 'structural', 'lemma_pattern': lemma_pattern, 'pos_pattern': pos_pattern}
Â  Â  Â  Â  pattern = re.escape(term.lower()).replace(r'\*', '.*')
Â  Â  Â  Â  return {'type': 'word', 'pattern': re.compile(f"^{pattern}$")}

Â  Â  search_components = [create_structural_matcher(term) for term in search_terms]
Â  Â  all_target_positions = []
Â  Â Â 
Â  Â  # --- NEW: Store the actual token that matched at the first position for breakdown ---
Â  Â  matching_tokens_at_node_one = []
Â  Â Â 
Â  Â  # Execute Search Loop (find all positions)
Â  Â  if primary_target_len == 1 and not is_structural_search:
Â  Â  Â  Â  target_pattern = search_components[0]['pattern']
Â  Â  Â  Â  for i, token in enumerate(tokens_lower):
Â  Â  Â  Â  Â  Â  if target_pattern.fullmatch(token):
Â  Â  Â  Â  Â  Â  Â  Â  all_target_positions.append(i)
Â  Â  Â  Â  Â  Â  Â  Â  matching_tokens_at_node_one.append(df_corpus['token'].iloc[i]) # Store the original token
Â  Â  else:
Â  Â  Â  Â  for i in range(len(tokens_lower) - primary_target_len + 1):
Â  Â  Â  Â  Â  Â  match = True
Â  Â  Â  Â  Â  Â  for k, component in enumerate(search_components):
Â  Â  Â  Â  Â  Â  Â  Â  corpus_index = i + k
Â  Â  Â  Â  Â  Â  Â  Â  if corpus_index >= len(df_corpus): break
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if component['type'] == 'word':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not component['pattern'].fullmatch(tokens_lower[corpus_index]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  match = False; break
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  elif component['type'] == 'structural':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_lemma = df_corpus["lemma"].iloc[corpus_index].lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_pos = df_corpus["pos"].iloc[corpus_index]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lemma_match = component['lemma_pattern'] is None or component['lemma_pattern'].fullmatch(current_lemma)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pos_match = component['pos_pattern'] is None or component['pos_pattern'].fullmatch(current_pos)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not (lemma_match and pos_match):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  match = False; break
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if match:
Â  Â  Â  Â  Â  Â  Â  Â  all_target_positions.append(i)
Â  Â  Â  Â  Â  Â  Â  Â  matching_tokens_at_node_one.append(df_corpus['token'].iloc[i]) # Store the original token (first word in MWU)
Â  Â  Â  Â  Â  Â Â 
Â  Â  literal_freq = len(all_target_positions)
Â  Â  if literal_freq == 0:
Â  Â  Â  Â  return ([], 0, raw_target_input, 0, [], pd.DataFrame())Â 
Â  Â  Â  Â Â 
Â  Â  # --- NEW: Generate Frequency Breakdown DataFrame ---
Â  Â  breakdown_data = Counter(matching_tokens_at_node_one)
Â  Â  breakdown_list = []
Â  Â  total_tokens_float = float(total_tokens)
Â  Â Â 
Â  Â  for token, freq in breakdown_data.most_common():
Â  Â  Â  Â  rel_freq = (freq / total_tokens_float) * 1_000_000
Â  Â  Â  Â  breakdown_list.append({
Â  Â  Â  Â  Â  Â  "Token Form": token,Â 
Â  Â  Â  Â  Â  Â  "Absolute Frequency": freq,Â 
Â  Â  Â  Â  Â  Â  "Relative Frequency (per M)": round(rel_freq, 4)
Â  Â  Â  Â  })
Â  Â  Â  Â Â 
Â  Â  breakdown_df = pd.DataFrame(breakdown_list)
Â  Â Â 
Â  Â  # --- NEW: Add Zipf Metrics (MODIFIED) ---
Â  Â  breakdown_df['Zipf Score'] = breakdown_df['Relative Frequency (per M)'].apply(pmw_to_zipf).round(2)
Â  Â  breakdown_df['Zipf Law Frequency Band'] = breakdown_df['Zipf Score'].apply(zipf_to_band)
Â  Â  # ----------------------------
Â  Â Â 
Â  Â  # ---------------------------------------------------
Â  Â  Â  Â Â 
Â  Â  # --- Apply Pattern Filtering ---
Â  Â  final_positions = all_target_positions
Â  Â Â 
Â  Â  # Check if a pattern filter is provided
Â  Â  is_pattern_search_active = pattern_collocate_input or pattern_collocate_pos_input

Â  Â  if is_pattern_search_active and pattern_window > 0:
Â  Â  Â  Â  final_positions = []
Â  Â  Â  Â  collocate_word_regex = re.compile(re.escape(pattern_collocate_input).replace(r'\*', '.*')) if pattern_collocate_input else None
Â  Â  Â  Â  collocate_pos_regex = NoneÂ 
Â  Â  Â  Â Â 
Â  Â  Â  Â  if pattern_collocate_pos_input and not is_raw_mode:
Â  Â  Â  Â  Â  Â  pos_patterns = [p.strip() for p in pattern_collocate_pos_input.split('|') if p.strip()]
Â  Â  Â  Â  Â  Â  if pos_patterns:
Â  Â  Â  Â  Â  Â  Â  Â  full_pos_regex = re.compile("^(" + "|".join([re.escape(p).replace(r'\*', '.*') for p in pos_patterns]) + ")$")
Â  Â  Â  Â  Â  Â  Â  Â  collocate_pos_regex = full_pos_regex

Â  Â  Â  Â  for i in all_target_positions:
Â  Â  Â  Â  Â  Â  start_index = max(0, i - pattern_window)
Â  Â  Â  Â  Â  Â  end_index = min(len(tokens_lower), i + primary_target_len + pattern_window)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  found_collocate = False
Â  Â  Â  Â  Â  Â  for j in range(start_index, end_index):
Â  Â  Â  Â  Â  Â  Â  Â  if i <= j < i + primary_target_len: continue # Skip node word(s)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  token_lower = tokens_lower[j]
Â  Â  Â  Â  Â  Â  Â  Â  token_pos = df_corpus["pos"].iloc[j] if "pos" in df_corpus.columns else '##'
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  word_matches = collocate_word_regex is None or collocate_word_regex.fullmatch(token_lower)
Â  Â  Â  Â  Â  Â  Â  Â  pos_matches = collocate_pos_regex is None or (collocate_pos_regex.fullmatch(token_pos) if not is_raw_mode else False)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if word_matches and pos_matches:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  found_collocate = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if found_collocate:
Â  Â  Â  Â  Â  Â  Â  Â  final_positions.append(i)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  total_matches = len(final_positions)
Â  Â  if total_matches == 0:
Â  Â  Â  Â  return ([], 0, raw_target_input, 0, [], breakdown_df)

Â  Â  # --- Sample Positions ---
Â  Â  if random_sample:
Â  Â  Â  Â  import random
Â  Â  Â  Â  random.seed(42) # Consistent random sample
Â  Â  Â  Â  sample_size = min(total_matches, limit)
Â  Â  Â  Â  display_positions = random.sample(final_positions, sample_size)
Â  Â  else:
Â  Â  Â  Â  display_positions = final_positions[:limit]
Â  Â Â 
Â  Â  # --- Format KWIC lines (MODIFIED for T/P/L inline display) ---
Â  Â  kwic_rows = []
Â  Â  sent_ids = [] # List to store the sentence ID for each KWIC row
Â  Â Â 
Â  Â  # Use pattern_window for context display if pattern search is active
Â  Â  current_kwic_left = pattern_window if is_pattern_search_active and pattern_window > 0 else kwic_left
Â  Â  current_kwic_right = pattern_window if is_pattern_search_active and pattern_window > 0 else kwic_right
Â  Â Â 
Â  Â  # Re-initialize regex for highlighting purposes (needs to be local)
Â  Â  collocate_word_regex_highlight = re.compile(re.escape(pattern_collocate_input).replace(r'\*', '.*')) if pattern_collocate_input else None
Â  Â Â 
Â  Â  # Re-generate POS regex if necessary
Â  Â  collocate_pos_regex_highlight = None
Â  Â  if pattern_collocate_pos_input and not is_raw_mode:
Â  Â  Â  Â  pos_patterns = [p.strip() for p in pattern_collocate_pos_input.split('|') if p.strip()]
Â  Â  Â  Â  if pos_patterns:
Â  Â  Â  Â  Â  Â  full_pos_regex = re.compile("^(" + "|".join([re.escape(p).replace(r'\*', '.*') for p in pos_patterns]) + ")$")
Â  Â  Â  Â  Â  Â  collocate_pos_regex_highlight = full_pos_regex

Â  Â Â 
Â  Â  # Helper to format a single token (inline format)
Â  Â  def format_token_inline(token, pos, lemma, is_collocate_match=False, is_node_word=False):
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- 1. Token (main font) ---
Â  Â  Â  Â  token_html = token
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Yellow background for collocate match
Â  Â  Â  Â  if is_collocate_match:
Â  Â  Â  Â  Â  Â  token_html = f"<span style='color: black; background-color: #FFEA00;'>{token}</span>"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Bold if it's the node word
Â  Â  Â  Â  if is_node_word:
Â  Â  Â  Â  Â  Â  Â token_html = f"<b>{token_html}</b>"
Â  Â  Â  Â Â 
Â  Â  Â  Â  output = [token_html]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. POS Tagging: /TAG
Â  Â  Â  Â  if show_pos:
Â  Â  Â  Â  Â  Â  pos_val = pos if pos not in ('##', '###') else ''
Â  Â  Â  Â  Â  Â  # Apply styling: smaller font, green color
Â  Â  Â  Â  Â  Â  styled_pos = f"<span style='font-size: 0.8em; color: #33CC33;'>{pos_val}</span>"
Â  Â  Â  Â  Â  Â  output.append("/" + styled_pos)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 3. Lemma Tagging: {lemma}
Â  Â  Â  Â  if show_lemma:
Â  Â  Â  Â  Â  Â  lemma_val = lemma if lemma not in ('##', '###') else ''
Â  Â  Â  Â  Â  Â  # Apply styling: smallest font, cyan color
Â  Â  Â  Â  Â  Â  styled_lemma = f"<span style='font-size: 0.7em; color: #00AAAA;'>{lemma_val}</span>"
Â  Â  Â  Â  Â  Â  output.append("{" + styled_lemma + "}")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  return "".join(output)


Â  Â  for i in display_positions:
Â  Â  Â  Â  kwic_start = max(0, i - current_kwic_left)
Â  Â  Â  Â  kwic_end = min(total_tokens, i + primary_target_len + current_kwic_right)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Determine the sentence ID of the node word (first token in MWU)
Â  Â  Â  Â  if is_parallel_mode and 'sent_id' in df_corpus.columns:
Â  Â  Â  Â  Â  Â  node_sent_id = df_corpus["sent_id"].iloc[i]
Â  Â  Â  Â  Â  Â  sent_ids.append(node_sent_id)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  sent_ids.append(None) # Not parallel mode or no ID available
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  formatted_line = []
Â  Â  Â  Â  node_orig_tokens = []
Â  Â  Â  Â  collocate_to_display = ""

Â  Â  Â  Â Â 
Â  Â  Â  Â  # Iterate over the context window
Â  Â  Â  Â  for k in range(kwic_start, kwic_end):
Â  Â  Â  Â  Â  Â  token = df_corpus["token"].iloc[k]
Â  Â  Â  Â  Â  Â  token_lower = df_corpus["_token_low"].iloc[k]
Â  Â  Â  Â  Â  Â  token_pos = df_corpus["pos"].iloc[k] if "pos" in df_corpus.columns else '##'
Â  Â  Â  Â  Â  Â  token_lemma = df_corpus["lemma"].iloc[k] if "lemma" in df_corpus.columns else '##'

Â  Â  Â  Â  Â  Â  is_node_word = (i <= k < i + primary_target_len)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  is_collocate_match = False
Â  Â  Â  Â  Â  Â  if is_pattern_search_active and not is_node_word:
Â  Â  Â  Â  Â  Â  Â  Â  Â word_matches_highlight = collocate_word_regex_highlight is None or collocate_word_regex_highlight.fullmatch(token_lower)
Â  Â  Â  Â  Â  Â  Â  Â  Â pos_matches_highlight = collocate_pos_regex_highlight is None or (collocate_pos_regex_highlight.fullmatch(token_pos) if not is_raw_mode else False)
Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â if word_matches_highlight and pos_matches_highlight:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_collocate_match = True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if collocate_to_display == "": # Capture the first matching collocate
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  collocate_to_display = token # Use the original token case
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if is_node_word:
Â  Â  Â  Â  Â  Â  Â  Â  # Format node word(s) as inline, explicitly marking them as node words
Â  Â  Â  Â  Â  Â  Â  Â  node_orig_tokens.append(format_token_inline(token, token_pos, token_lemma, is_collocate_match=False, is_node_word=True))Â 
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # Format context tokens (potentially collocates)
Â  Â  Â  Â  Â  Â  Â  Â  formatted_line.append(format_token_inline(token, token_pos, token_lemma, is_collocate_match=is_collocate_match, is_node_word=False))


Â  Â  Â  Â  # FIX 2: Corrected slicing logic
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. Determine the number of tokens in the left context
Â  Â  Â  Â  # This is the index 'i' relative to 'kwic_start'
Â  Â  Â  Â  left_context_count = i - kwic_start
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. Slice the context tokens (formatted_line) correctly:
Â  Â  Â  Â  left_context = formatted_line[:left_context_count]
Â  Â  Â  Â  right_context = formatted_line[left_context_count:]

Â  Â  Â  Â  node_orig = " ".join(node_orig_tokens)Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Join the token groups by a *single* space
Â  Â  Â  Â  kwic_rows.append({
Â  Â  Â  Â  Â  Â  "Left": " ".join(left_context),Â 
Â  Â  Â  Â  Â  Â  "Node": node_orig,Â 
Â  Â  Â  Â  Â  Â  "Right": " ".join(right_context),
Â  Â  Â  Â  Â  Â  "Collocate": collocate_to_display # Only filled if pattern search is active
Â  Â  Â  Â  })
Â  Â  Â  Â Â 
Â  Â  return (kwic_rows, total_matches, raw_target_input, literal_freq, sent_ids, breakdown_df) # Added breakdown_df

# --- N-GRAM LOGIC FUNCTION (FIXED: Added corpus_id argument for better caching) ---
@st.cache_data(show_spinner=False)
def generate_n_grams(df_corpus, n_size, n_gram_filters, is_raw_mode, corpus_id):
Â  Â  """
Â  Â  Generates N-grams, applies positional filters (token, POS, lemma), and calculates frequencies.
Â  Â  corpus_id is included in the signature to ensure cache invalidation when the corpus changes.
Â  Â  """
Â  Â  total_tokens = len(df_corpus)
Â  Â  if total_tokens < n_size or n_size < 1:
Â  Â  Â  Â  return pd.DataFrame()
Â  Â Â 
Â  Â  # Pre-extract lists for faster lookup
Â  Â  tokens = df_corpus["token"].tolist()
Â  Â  tokens_low = df_corpus["_token_low"].tolist()
Â  Â  pos = df_corpus["pos"].tolist() if "pos" in df_corpus.columns else ["##"] * total_tokens
Â  Â  lemma = df_corpus["lemma"].tolist() if "lemma" in df_corpus.columns else ["##"] * total_tokens

Â  Â  def matches_filter(token, token_low, pos_tag, lemma_tag, pattern_str, is_raw_mode):
Â  Â  Â  Â  """Checks if a single token/tag set matches a positional pattern string."""
Â  Â  Â  Â  if not pattern_str:
Â  Â  Â  Â  Â  Â  return True

Â  Â  Â  Â  pattern_str = pattern_str.strip()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. Structural/Lemma Query ([lemma*])
Â  Â  Â  Â  lemma_match_re = re.search(r"\[(.*?)\]", pattern_str)
Â  Â  Â  Â  if lemma_match_re and not is_raw_mode:
Â  Â  Â  Â  Â  Â  lemma_pattern = re.escape(lemma_match_re.group(1).lower()).replace(r'\*', '.*')
Â  Â  Â  Â  Â  Â  return re.fullmatch(f"^{lemma_pattern}$", lemma_tag.lower())

Â  Â  Â  Â  # 2. POS Query (_POS*)
Â  Â  Â  Â  pos_match_re = re.search(r"\_([\w\*|]+)", pattern_str)
Â  Â  Â  Â  if pos_match_re and not is_raw_mode:
Â  Â  Â  Â  Â  Â  pos_input = pos_match_re.group(1).strip()
Â  Â  Â  Â  Â  Â  pos_patterns = [p.strip() for p in pos_input.split('|') if p.strip()]
Â  Â  Â  Â  Â  Â  full_pos_regex_list = [re.escape(p).replace(r'\*', '.*') for p in pos_patterns]
Â  Â  Â  Â  Â  Â  full_pos_regex = re.compile("^(" + "|".join(full_pos_regex_list) + ")$")
Â  Â  Â  Â  Â  Â  return full_pos_regex.fullmatch(pos_tag)

Â  Â  Â  Â  # 3. Simple Token/Word Query (word*)
Â  Â  Â  Â  pattern = re.escape(pattern_str).replace(r'\*', '.*')
Â  Â  Â  Â  return re.fullmatch(f"^{pattern}$", token_low)

Â  Â  Â  Â Â 
Â  Â  matched_n_grams_list = []
Â  Â Â 
Â  Â  for i in range(total_tokens - n_size + 1):
Â  Â  Â  Â  current_tokens = tokens[i:i + n_size]
Â  Â  Â  Â  current_tokens_low = tokens_low[i:i + n_size]
Â  Â  Â  Â  current_pos = pos[i:i + n_size]
Â  Â  Â  Â  current_lemma = lemma[i:i + n_size]
Â  Â  Â  Â Â 
Â  Â  Â  Â  is_match = True
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Apply positional filters
Â  Â  Â  Â  for pos_idx, pattern_str in n_gram_filters.items():
Â  Â  Â  Â  Â  Â  pos_int = int(pos_idx) - 1 # Convert 1-based UI index to 0-based Python index
Â  Â  Â  Â  Â  Â  if pos_int < 0 or pos_int >= n_size: continue
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not matches_filter(
Â  Â  Â  Â  Â  Â  Â  Â  current_tokens[pos_int],Â 
Â  Â  Â  Â  Â  Â  Â  Â  current_tokens_low[pos_int],Â 
Â  Â  Â  Â  Â  Â  Â  Â  current_pos[pos_int],Â 
Â  Â  Â  Â  Â  Â  Â  Â  current_lemma[pos_int],Â 
Â  Â  Â  Â  Â  Â  Â  Â  pattern_str,Â 
Â  Â  Â  Â  Â  Â  Â  Â  is_raw_mode
Â  Â  Â  Â  Â  Â  ):
Â  Â  Â  Â  Â  Â  Â  Â  is_match = False
Â  Â  Â  Â  Â  Â  Â  Â  break
Â  Â  Â  Â Â 
Â  Â  Â  Â  if is_match:
Â  Â  Â  Â  Â  Â  matched_n_grams_list.append(tuple(current_tokens))
Â  Â  Â  Â  Â  Â Â 
Â  Â  if not matched_n_grams_list:
Â  Â  Â  Â  return pd.DataFrame()
Â  Â  Â  Â Â 
Â  Â  # Count frequencies
Â  Â  n_gram_counts = Counter(matched_n_grams_list)
Â  Â Â 
Â  Â  data = []
Â  Â  total_tokens_float = float(total_tokens) # Use float for accurate calculation
Â  Â Â 
Â  Â  for n_gram, freq in n_gram_counts.items():
Â  Â  Â  Â  n_gram_str = " ".join(n_gram)
Â  Â  Â  Â  # Calculate relative frequency per million tokens
Â  Â  Â  Â  rel_freq = (freq / total_tokens_float) * 1_000_000
Â  Â  Â  Â Â 
Â  Â  Â  Â  data.append({
Â  Â  Â  Â  Â  Â  "N-Gram": n_gram_str,
Â  Â  Â  Â  Â  Â  "Frequency": freq,
Â  Â  Â  Â  Â  Â  "Relative Frequency (per M)": round(rel_freq, 4)
Â  Â  Â  Â  })
Â  Â  Â  Â Â 
Â  Â  n_gram_df = pd.DataFrame(data)
Â  Â Â 
Â  Â  def is_only_punc_or_digit(n_gram_str):
Â  Â  Â  Â  for token in n_gram_str.split():
Â  Â  Â  Â  Â  Â  if token.lower() not in PUNCTUATION and not token.isdigit():
Â  Â  Â  Â  Â  Â  Â  Â  return False
Â  Â  Â  Â  return True
Â  Â  Â  Â Â 
Â  Â  n_gram_df = n_gram_df[~n_gram_df["N-Gram"].apply(is_only_punc_or_digit)]
Â  Â Â 
Â  Â  return n_gram_df.sort_values("Frequency", ascending=False).reset_index(drop=True)
# -----------------------------

# --- Statistical Helpers ---
EPS = 1e-12
def safe_log(x):
Â  Â  return math.log(max(x, EPS))
def compute_ll(k11, k12, k21, k22):
Â  Â  """Computes the Log-Likelihood (LL) statistic."""
Â  Â  total = k11 + k12 + k21 + k22
Â  Â  if total == 0: return 0.0
Â  Â  e11 = (k11 + k12) * (k11 + k21) / total
Â  Â  e12 = (k11 + k12) * (k12 + k22) / total
Â  Â  e21 = (k21 + k22) * (k11 + k21) / total
Â  Â  e22 = (k21 + k22) * (k12 + k22) / total
Â  Â  s = 0.0
Â  Â  for k,e in ((k11,e11),(k12,e12),(k21,e21),(k22,e22)):
Â  Â  Â  Â  if k > 0 and e > 0: s += k * math.log(k / e)
Â  Â  return 2.0 * s
def compute_mi(k11, target_freq, coll_total, corpus_size):
Â  Â  """Compuutes the Mutual Information (MI) statistic."""
Â  Â  expected = (target_freq * coll_total) / corpus_size
Â  Â  if expected == 0 or k11 == 0: return 0.0
Â  Â  return math.log2(k11 / expected)
def significance_from_ll(ll_val):
Â  Â  """Converts Log-Likelihood value to significance level."""
Â  Â  if ll_val >= 15.13: return '*** (p<0.001)'
Â  Â  if ll_val >= 10.83: return '** (p<0.01)'
Â  Â  if ll_val >= 3.84: return ' * (p<0.05)'
Â  Â  return 'ns'

# --- IO / Data Helpers ---
def df_to_excel_bytes(df):
Â  Â  buf = BytesIO()
Â  Â  with pd.ExcelWriter(buf, engine="xlsxwriter") as writer:
Â  Â  Â  Â  df.to_excel(writer, index=False, sheet_name="Sheet1")
Â  Â  buf.seek(0)
Â  Â  return buf.getvalue()

@st.cache_data
def create_pyvis_graph(target_word, coll_df):
Â  Â  if not PYVIS_FEATURE_AVAILABLE: return ""

Â  Â  net = Network(height="400px", width="100%", bgcolor="#222222", font_color="white", cdn_resources='local')
Â  Â  if coll_df.empty: return ""
Â  Â  max_ll = coll_df['LL'].max()
Â  Â  min_ll = coll_df['LL'].min()
Â  Â  ll_range = max_ll - min_ll
Â  Â Â 
Â  Â  net.set_options("""
Â  Â  var options = {
Â  Â  Â  "nodes": {"borderWidth": 2, "size": 15, "font": {"size": 30}},
Â  Â  Â  "edges": {"width": 5, "smooth": {"type": "dynamic"}},
Â  Â  Â  "physics": {"barnesHut": {"gravitationalConstant": -10000, "centralGravity": 0.3, "springLength": 95, "springConstant": 0.04, "damping": 0.9, "avoidOverlap": 0.5}, "minVelocity": 0.75}
Â  Â  }
Â  Â  """)
Â  Â Â 
Â  Â  net.add_node(target_word, label=target_word, size=40, color='#FFFF00', title=f"Target: {target_word}", x=0, y=0, fixed=True, font={'color': 'black'})
Â  Â Â 
Â  Â  LEFT_BIAS = -500; RIGHT_BIAS = 500
Â  Â  all_directions = coll_df['Direction'].unique()
Â  Â  if 'R' not in all_directions and 'L' in all_directions: RIGHT_BIAS = -500
Â  Â  elif 'L' not in all_directions and 'R' in all_directions: LEFT_BIAS = 500

Â  Â  for index, row in coll_df.iterrows():
Â  Â  Â  Â  collocate = row['Collocate']
Â  Â  Â  Â  ll_score = row['LL']
Â  Â  Â  Â  observed = row['Observed']
Â  Â  Â  Â  pos_tag = row['POS']
Â  Â  Â  Â  direction = row.get('Direction', 'R')Â 
Â  Â  Â  Â  obs_l = row.get('Obs_L', 0)
Â  Â  Â  Â  obs_r = row.get('Obs_R', 0)
Â  Â  Â  Â  x_position = LEFT_BIAS if direction in ('L', 'B') else RIGHT_BIAS

Â  Â  Â  Â  pos_code = pos_tag[0].upper() if pos_tag and len(pos_tag) > 0 else 'O'
Â  Â  Â  Â  if pos_tag.startswith('##'): pos_code = '#'
Â  Â  Â  Â  elif pos_code not in ['N', 'V', 'J', 'R']: pos_code = 'O'
Â  Â  Â  Â Â 
Â  Â  Â  Â  color = POS_COLOR_MAP.get(pos_code, POS_COLOR_MAP['O'])
Â  Â  Â  Â Â 
Â  Â  Â  Â  node_size = 25
Â  Â  Â  Â  if ll_range > 0:
Â  Â  Â  Â  Â  Â  normalized_ll = (ll_score - min_ll) / ll_range
Â  Â  Â  Â  Â  Â  node_size = 15 + normalized_ll * 25Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  tooltip_title = (
Â  Â  Â  Â  Â  Â  f"POS: {row['POS']}\n"
Â  Â  Â  Â  Â  Â  f"Obs: {observed} (Left: {obs_l}, Right: {obs_r})\n"
Â  Â  Â  Â  Â  Â  f"LL: {ll_score:.2f}\n"
Â  Â  Â  Â  Â  Â  f"Dominant Direction: {direction}"
Â  Â  Â  Â  )

Â  Â  Â  Â  net.add_node(collocate, label=collocate, size=node_size, color=color, title=tooltip_title, x=x_position)
Â  Â  Â  Â  net.add_edge(target_word, collocate, value=ll_score, width=5, title=f"LL: {ll_score:.2f}")

Â  Â  html_content = ""; temp_path = None
Â  Â  try:
Â  Â  Â  Â  temp_filename = "pyvis_graph.html"
Â  Â  Â  Â  temp_dir = tempfile.gettempdir()
Â  Â  Â  Â  temp_path = os.path.join(temp_dir, temp_filename)
Â  Â  Â  Â  net.write_html(temp_path, notebook=False)
Â  Â  Â  Â  with open(temp_path, 'r', encoding='utf-8') as f: html_content = f.read()
Â  Â  finally:
Â  Â  Â  Â  if temp_path and os.path.exists(temp_path): os.remove(temp_path)

Â  Â  return html_content

@st.cache_data
def download_file_to_bytesio(url):
Â  Â  try:
Â  Â  Â  Â  response = requests.get(url, stream=True)
Â  Â  Â  Â  response.raise_for_status()Â 
Â  Â  Â  Â  return BytesIO(response.content)
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Failed to download built-in corpus from {url}. Ensure the file is public and the URL is a RAW content link.")
Â  Â  Â  Â  return None

# --- NEW: Robust XML Sanitization Helper ---
def sanitize_xml_content(file_source):
Â  Â  """
Â  Â  Reads file content, performs robust cleaning for control charactersÂ 
Â  Â  and unescaped entities, and returns the cleaned string.
Â  Â  """
Â  Â  file_source.seek(0)
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  xml_content_bytes = file_source.read()
Â  Â  Â  Â  xml_content = xml_content_bytes.decode('utf-8')
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 1. Remove illegal control characters (keep \t, \n, \r)
Â  Â  Â  Â  # XML 1.0 valid chars: #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]
Â  Â  Â  Â  # Python's re.sub below handles the main C0/C1 control blocks
Â  Â  Â  Â  illegal_chars_re = re.compile(u'[^\u0020-\uD7FF\uE000-\uFFFD\u0009\u000A\u000D]', re.IGNORECASE)
Â  Â  Â  Â  cleaned_xml_content = illegal_chars_re.sub('', xml_content)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. Fix unescaped ampersands (&) that are not part of an existing entity reference (e.g., &amp;)
Â  Â  Â  Â  cleaned_xml_content = re.sub(r'&(?![A-Za-z0-9#]{2,5};|#)', r'&amp;', cleaned_xml_content)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 3. Aggressive: Remove leading/trailing whitespace which sometimes confuses parsers
Â  Â  Â  Â  cleaned_xml_content = cleaned_xml_content.strip()
Â  Â  Â  Â Â 
Â  Â  Â  Â  return cleaned_xml_content
Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  st.session_state['xml_structure_error'] = f"File Read/Decode Error: {e}"
Â  Â  Â  Â  return None
# ----------------------------------------------


# ---------------------------------------------------------------------
# XML PARSING HELPERS (Refactored to use sanitize_xml_content)
# ---------------------------------------------------------------------

def extract_xml_structure(file_source, max_values=20):
Â  Â  """
Â  Â  Parses an XML file and extracts structure, using ET.fromstring on the cleaned content.
Â  Â  """
Â  Â  if file_source is None:
Â  Â  Â  Â  return None
Â  Â Â 
Â  Â  if isinstance(file_source, list):
Â  Â  Â  Â  if not file_source: return None
Â  Â  Â  Â  file_to_analyze = file_source[0]
Â  Â  else:
Â  Â  Â  Â  file_to_analyze = file_source

Â  Â  # Apply aggressive sanitization first
Â  Â  cleaned_xml_content = sanitize_xml_content(file_to_analyze)
Â  Â Â 
Â  Â  if cleaned_xml_content is None:
Â  Â  Â  Â  return None # Error already captured in session state if sanitization/read failed

Â  Â  # --- AGGRESSIVE ERROR CAPTURE ---
Â  Â  try:
Â  Â  Â  Â  # Parse from string (after cleaning)
Â  Â  Â  Â  root = ET.fromstring(cleaned_xml_content)Â 
Â  Â  Â  Â  st.session_state['xml_structure_error'] = None # Clear previous error on success
Â  Â  except Exception as e:
Â  Â  Â  Â  # Store the exact parsing error string in session state
Â  Â  Â  Â  st.session_state['xml_structure_error'] = f"XML Parsing Error: {e}"
Â  Â  Â  Â  return None
Â  Â  # --- END AGGRESSIVE ERROR CAPTURE ---

Â  Â  # Structure: {tag_name: {attr_name: set_of_values, ...}, ...}
Â  Â  structure = {}
Â  Â Â 
Â  Â  def process_element(element):
Â  Â  Â  Â  tag = element.tag
Â  Â  Â  Â  if tag not in structure:
Â  Â  Â  Â  Â  Â  structure[tag] = {}
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Process attributes
Â  Â  Â  Â  for attr_name, attr_value in element.attrib.items():
Â  Â  Â  Â  Â  Â  if attr_name not in structure[tag]:
Â  Â  Â  Â  Â  Â  Â  Â  structure[tag][attr_name] = set()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Sample unique values up to max_values
Â  Â  Â  Â  Â  Â  if len(structure[tag][attr_name]) < max_values:
Â  Â  Â  Â  Â  Â  Â  Â  structure[tag][attr_name].add(attr_value)

Â  Â  Â  Â  # Recurse through children
Â  Â  Â  Â  for child in element:
Â  Â  Â  Â  Â  Â  process_element(child)

Â  Â  process_element(root)
Â  Â Â 
Â  Â  return structure

# Helper to format structure data into an indented HTML list (NEW)
def format_structure_data_hierarchical(structure_data, indent_level=0, max_values=20):
Â  Â  """
Â  Â  Formats the hierarchical XML structure data into an indented HTML list.
Â  Â  """
Â  Â  if not structure_data:
Â  Â  Â  Â  return ""

Â  Â  html_list = []
Â  Â Â 
Â  Â  # Helper for indentation and basic styling
Â  Â  def get_indent(level):
Â  Â  Â  Â  # 1.5em per level for indentation
Â  Â  Â  Â  return f'<span style="padding-left: {level * 1.5}em;">'

Â  Â  # Sort tags alphabetically for consistent display
Â  Â  for tag in sorted(structure_data.keys()):
Â  Â  Â  Â  tag_data = structure_data[tag]
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Start the tag line
Â  Â  Â  Â  tag_line = f'{get_indent(indent_level)}<span style="color: #6A5ACD; font-weight: bold;">&lt;{tag}&gt;</span></span><br>'
Â  Â  Â  Â  html_list.append(tag_line)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Process attributes for this tag
Â  Â  Â  Â  for attr in sorted(tag_data.keys()):
Â  Â  Â  Â  Â  Â  values = sorted(list(tag_data.get(attr, set())))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Format sampled values
Â  Â  Â  Â  Â  Â  sampled_values_str = ", ".join(values[:max_values])
Â  Â  Â  Â  Â  Â  if len(values) > max_values:
Â  Â  Â  Â  Â  Â  Â  Â  sampled_values_str += f", ... ({len(values) - max_values} more unique)"

Â  Â  Â  Â  Â  Â  # Attribute line: indented, showing attribute name and sampled values
Â  Â  Â  Â  Â  Â  attr_line = f'{get_indent(indent_level + 1)}'
Â  Â  Â  Â  Â  Â  attr_line += f'<span style="color: #8B4513;">@{attr}</span> = '
Â  Â  Â  Â  Â  Â  attr_line += f'<span style="color: #3CB371;">"{sampled_values_str}"</span></span><br>'
Â  Â  Â  Â  Â  Â  html_list.append(attr_line)

Â  Â  return "".join(html_list)


# Core function to parse XML and extract tokens (used by both monolingual and parallel loaders)
def parse_xml_content_to_df(file_source):
Â  Â  """
Â  Â  Parses a single XML file, extracts sentences and IDs, and tokenizes/verticalizes if needed.
Â  Â  Returns: {'lang_code': str, 'df_data': list of dicts, 'sent_map': {sent_id: raw_sentence_text}}
Â  Â  """
Â  Â Â 
Â  Â  cleaned_xml_content = sanitize_xml_content(file_source)
Â  Â Â 
Â  Â  if cleaned_xml_content is None:
Â  Â  Â  Â  return None
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  # Use fromstring for robustness after cleaning
Â  Â  Â  Â  root = ET.fromstring(cleaned_xml_content)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 2. Extract Language Code: Check corpus > text > root attributes
Â  Â  Â  Â  lang_code = root.get('lang')
Â  Â  Â  Â  if not lang_code:
Â  Â  Â  Â  Â  Â  # Look for lang attribute in <text> or <corpus> tag in the raw string
Â  Â  Â  Â  Â  Â  lang_match = re.search(r'(<text\s+lang="([^"]+)">|<corpus\s+[^>]*lang="([^"]+)">)', cleaned_xml_content)
Â  Â  Â  Â  Â  Â  if lang_match:
Â  Â  Â  Â  Â  Â  Â  Â  # Group 2 is from <text>, Group 3 is from <corpus> (prioritize <corpus>)
Â  Â  Â  Â  Â  Â  Â  Â  lang_code = lang_match.group(3) or lang_match.group(2)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  if not lang_code:
Â  Â  Â  Â  Â  Â  # Default to XML if no language code is explicitly found
Â  Â  Â  Â  Â  Â  lang_code = 'XML'Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  lang_code = lang_code.upper()
Â  Â  Â  Â  Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  # Critical failure: XML is not well-formed even after cleaning
Â  Â  Â  Â  file_name_label = getattr(file_source, 'name', 'Uploaded XML File')
Â  Â  Â  Â  st.error(f"Error reading or parsing XML file {file_name_label}: {e}")
Â  Â  Â  Â  st.session_state['xml_structure_error'] = f"Tokenization Parse Error: {e}" # Ensure tokenization error is also visible
Â  Â  Â  Â  return None

Â  Â  df_data = []
Â  Â  sent_map = {}
Â  Â Â 
Â  Â  # 3. Iterate over <sent> tags (or similar, like <p> if no <sent> is found)
Â  Â  sent_tags = root.findall('sent')
Â  Â  if not sent_tags: # Fallback to looking at direct children if <sent> is missing (e.g., if the user uses <p>)
Â  Â  Â  Â  sent_tags = list(root)
Â  Â Â 
Â  Â  if not sent_tags:
Â  Â  Â  Â  # Fallback 1: Try to process *all* text content in root
Â  Â  Â  Â  raw_sentence_text = "".join(root.itertext()).strip()Â 
Â  Â  Â  Â  if raw_sentence_text:
Â  Â  Â  Â  Â  Â  # Tokenize the entire raw text
Â  Â  Â  Â  Â  Â  cleaned_text = re.sub(r'([^\w\s])', r' \1 ', raw_sentence_text)Â 
Â  Â  Â  Â  Â  Â  tokens = [t.strip() for t in cleaned_text.split() if t.strip()]
Â  Â  Â  Â  Â  Â  if tokens:
Â  Â  Â  Â  Â  Â  Â  Â for token in tokens:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_data.append({"token": token, "pos": "##", "lemma": "##", "sent_id": 1})
Â  Â  Â  Â  Â  Â  Â  Â sent_map[1] = raw_sentence_text
Â  Â  Â  Â  Â  Â  return {'lang_code': lang_code, 'df_data': df_data, 'sent_map': sent_map}
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  file_name_label = getattr(file_source, 'name', 'Uploaded XML File')
Â  Â  Â  Â  st.warning(f"No parseable content found in corpus file: {file_name_label}.")
Â  Â  Â  Â  return None

Â  Â  # --- Use a counter for missing/non-integer IDs for robustness ---
Â  Â  sequential_id_counter = 0

Â  Â  for sent_elem in sent_tags:
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- ID Extraction: Prioritize 'n' > 'id' > Sequential Counter ---
Â  Â  Â  Â  sent_id_str = sent_elem.get('n') or sent_elem.get('id')
Â  Â  Â  Â Â 
Â  Â  Â  Â  sent_id = None
Â  Â  Â  Â Â 
Â  Â  Â  Â  if sent_id_str:
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # Try to convert to integer (required for alignment checks)
Â  Â  Â  Â  Â  Â  Â  Â  sent_id = int(sent_id_str)
Â  Â  Â  Â  Â  Â  except ValueError:
Â  Â  Â  Â  Â  Â  Â  Â  # If the ID is a string (e.g., "s1.2") or simply non-numeric, use sequential
Â  Â  Â  Â  Â  Â  Â  Â  sequential_id_counter += 1
Â  Â  Â  Â  Â  Â  Â  Â  sent_id = sequential_id_counter
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # If no 'n' or 'id' attribute found, use sequential ID.
Â  Â  Â  Â  Â  Â  sequential_id_counter += 1
Â  Â  Â  Â  Â  Â  sent_id = sequential_id_counter

Â  Â  Â  Â  if sent_id is None:Â 
Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  # --- Check for nested <w> tags (Vertical/Tagged Format) ---
Â  Â  Â  Â  word_tags = sent_elem.findall('.//w') # Use findall('.//w') to search recursively
Â  Â  Â  Â Â 
Â  Â  Â  Â  raw_sentence_text = ""
Â  Â  Â  Â Â 
Â  Â  Â  Â  if word_tags:
Â  Â  Â  Â  Â  Â  # Tagged XML format (e.g., TreeTagger/TEI-like)
Â  Â  Â  Â  Â  Â  raw_tokens = []
Â  Â  Â  Â  Â  Â  for w_elem in word_tags:
Â  Â  Â  Â  Â  Â  Â  Â  token = w_elem.text.strip() if w_elem.text else ""
Â  Â  Â  Â  Â  Â  Â  Â  pos = w_elem.get('pos') or w_elem.get('type') or "##"
Â  Â  Â  Â  Â  Â  Â  Â  lemma = w_elem.get('lemma') or "##"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if not token: continue
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  df_data.append({"token": token, "pos": pos, "lemma": lemma, "sent_id": sent_id})
Â  Â  Â  Â  Â  Â  Â  Â  raw_tokens.append(token)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  raw_sentence_text = " ".join(raw_tokens)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  # Raw Text XML format (Linear) - content is inside the <sent> tag itself
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # FIX: Use itertext() to extract all text content robustly, ignoring child tags/attributes
Â  Â  Â  Â  Â  Â  raw_sentence_text = "".join(sent_elem.itertext()).strip()Â 
Â  Â  Â  Â  Â  Â  inner_content = raw_sentence_text
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Check for embedded vertical format (multi-line, multi-column data *inside* the tag)
Â  Â  Â  Â  Â  Â  normalized_content = inner_content.replace('\r\n', '\n').replace('\r', '\n')
Â  Â  Â  Â  Â  Â  lines = [line.strip() for line in normalized_content.split('\n') if line.strip()]
Â  Â  Â  Â  Â  Â  # FIX: Corrected Syntax Error in re.split pattern
Â  Â  Â  Â  Â  Â  is_vertical_format = sum(line.count('\t') > 0 or len(re.split(r'\s+', line.strip())) >= 3 for line in lines) / len(lines) > 0.5
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if is_vertical_format:
Â  Â  Â  Â  Â  Â  Â  Â  raw_tokens = []
Â  Â  Â  Â  Â  Â  Â  Â  for line in lines:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parts = re.split(r'\s+', line.strip(), 2)Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  token = parts[0]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pos = parts[1] if len(parts) > 1 and parts[1] else "##"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lemma = parts[2] if len(parts) > 2 and parts[2] else "##"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not token: continue
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_data.append({"token": token, "pos": pos, "lemma": lemma, "sent_id": sent_id})
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  raw_tokens.append(token)
Â  Â  Â  Â  Â  Â  Â  Â  # Keep raw_sentence_text as is (extracted via itertext()) for the sent_map
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  # Pure Horizontal text (raw) - requires tokenization
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # Use the clean raw_sentence_text derived from itertext()
Â  Â  Â  Â  Â  Â  Â  Â  raw_text_to_tokenize = raw_sentence_text.replace('\n', ' ').replace('\t', ' ')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # --- FIXED TOKENIZATION ---
Â  Â  Â  Â  Â  Â  Â  Â  # 1. Add spaces around punctuation/symbolsÂ 
Â  Â  Â  Â  Â  Â  Â  Â  cleaned_text = re.sub(r'([^\w\s])', r' \1 ', raw_text_to_tokenize)Â 
Â  Â  Â  Â  Â  Â  Â  Â  # 2. Split by any whitespace that remains
Â  Â  Â  Â  Â  Â  Â  Â  tokens = [t.strip() for t in cleaned_text.split() if t.strip()]Â 
Â  Â  Â  Â  Â  Â  Â  Â  # --------------------------

Â  Â  Â  Â  Â  Â  Â  Â  for token in tokens:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_data.append({"token": token, "pos": "##", "lemma": "##", "sent_id": sent_id})
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Store raw sentence for the target map
Â  Â  Â  Â  if raw_sentence_text:
Â  Â  Â  Â  Â  Â  sent_map[sent_id] = raw_sentence_text.strip()
Â  Â  Â  Â  Â  Â Â 
Â  Â  if not df_data:
Â  Â  Â  Â  file_name_label = getattr(file_source, 'name', 'Uploaded XML File')
Â  Â  Â  Â  st.warning(f"No tokenized data was extracted from the XML file: {file_name_label}.")
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  return {'lang_code': lang_code, 'df_data': df_data, 'sent_map': sent_map}


# ---------------------------------------------------------------------
# Monolingual XML LoaderÂ 
# ---------------------------------------------------------------------
@st.cache_data
def load_monolingual_corpus_files(file_sources, explicit_lang_code, selected_format):
Â  Â  global SOURCE_LANG_CODE, TARGET_LANG_CODE
Â  Â Â 
Â  Â  st.session_state['parallel_mode'] = False
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
Â  Â  st.session_state['target_sent_map'] = {}
Â  Â  st.session_state['xml_structure_data'] = None # Reset old structure
Â  Â  st.session_state['xml_structure_error'] = None # Reset old error

Â  Â  if not file_sources:
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  all_df_data = []
Â  Â Â 
Â  Â  # Set the global language code from the user's explicit selection initially (This is fine for initial state before XML parsing)
Â  Â  SOURCE_LANG_CODE = explicit_lang_code
Â  Â  TARGET_LANG_CODE = 'NA'
Â  Â Â 
Â  Â  is_tagged_format = 'verticalised' in selected_format or 'TreeTagger' in selected_format
Â  Â Â 
Â  Â  # Track the language detected by the XML parser if explicit code was 'OTHER'
Â  Â  xml_detected_lang_code = None

Â  Â Â 
Â  Â  for file_source in file_sources:
Â  Â  Â  Â  file_source.seek(0)
Â  Â  Â  Â Â 
Â  Â  Â  Â  if file_source.name.lower().endswith('.xml'):
Â  Â  Â  Â  Â  Â  result = parse_xml_content_to_df(file_source)
Â  Â  Â  Â  Â  Â  if result:
Â  Â  Â  Â  Â  Â  Â  Â  # If user chose 'OTHER', we cache the XML detected code for the global update later
Â  Â  Â  Â  Â  Â  Â  Â  if explicit_lang_code == 'OTHER' and result['lang_code'] not in ('XML', 'OTHER'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xml_detected_lang_code = result['lang_code']Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  all_df_data.extend(result['df_data'])
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['monolingual_xml_file_upload'] = file_sourceÂ 
Â  Â  Â  Â Â 
Â  Â  Â  Â  else: # TXT, CSV, or assumed RAW (non-XML)
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  file_bytes = file_source.read()
Â  Â  Â  Â  Â  Â  Â  Â  file_content_str = file_bytes.decode('utf-8', errors='ignore')
Â  Â  Â  Â  Â  Â  Â  Â  clean_lines = [line for line in file_content_str.splitlines() if line and not line.strip().startswith('#')]
Â  Â  Â  Â  Â  Â  Â  Â  clean_content = "\n".join(clean_lines)
Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"Error reading raw file content: {e}")
Â  Â  Â  Â  Â  Â  Â  Â  continue

Â  Â  Â  Â  Â  Â  # Check if it is assumed to be a vertical T/P/L file
Â  Â  Â  Â  Â  Â  if is_tagged_format:
Â  Â  Â  Â  Â  Â  Â  Â  file_buffer_for_pandas = StringIO(clean_content)
Â  Â  Â  Â  Â  Â  Â  Â  df_attempt = None
Â  Â  Â  Â  Â  Â  Â  Â  for sep_char in ['\t', r'\s+']:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_buffer_for_pandas.seek(0)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_attempt = pd.read_csv(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_buffer_for_pandas,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sep=sep_char,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  header=None,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  engine="python",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  dtype=str,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  skipinitialspace=True,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  usecols=[0, 1, 2],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  names=['token', 'pos', 'lemma']
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df_attempt is not None and df_attempt.shape[1] >= 3:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  breakÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_attempt = NoneÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_attempt = NoneÂ 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if df_attempt is not None and df_attempt.shape[1] >= 3:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_file = df_attempt.copy()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_file["token"] = df_file["token"].fillna("").astype(str).str.strip()Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_file["pos"] = df_file["pos"].fillna("###").astype(str)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_file["lemma"] = df_file["lemma"].fillna("###").astype(str)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_file['sent_id'] = 0Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  all_df_data.extend(df_file.to_dict('records'))
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning(f"File {file_source.name} was expected to be a vertical format but could not be parsed as 3+ columns. Falling back to raw text.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  is_tagged_format = False # Fallback to raw for this file
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not is_tagged_format or selected_format == '.txt': # Raw Text Processing
Â  Â  Â  Â  Â  Â  Â  Â  raw_text = clean_content
Â  Â  Â  Â  Â  Â  Â  Â  cleaned_text = re.sub(r'([^\w\s])', r' \1 ', raw_text)
Â  Â  Â  Â  Â  Â  Â  Â  tokens = [t.strip() for t in cleaned_text.split() if t.strip()]Â 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  df_raw_file = pd.DataFrame({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "token": tokens,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "pos": ["##"] * len(tokens),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "lemma": ["##"] * len(tokens),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "sent_id": [0] * len(tokens)
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â  Â  Â  all_df_data.extend(df_raw_file.to_dict('records'))

Â  Â  if not all_df_data:
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  df_src = pd.DataFrame(all_df_data)
Â  Â  df_src["_token_low"] = df_src["token"].str.lower()
Â  Â Â 
Â  Â  # If XML detection occurred and user chose 'OTHER', update SOURCE_LANG_CODE
Â  Â  if xml_detected_lang_code:
Â  Â  Â  Â  SOURCE_LANG_CODE = xml_detected_lang_code # Update global var inside the cached function if auto-detected
Â  Â Â 
Â  Â  # Structure extraction for the first file (if XML)
Â  Â  if st.session_state['monolingual_xml_file_upload']:
Â  Â  Â  Â  st.session_state['xml_structure_data'] = extract_xml_structure(st.session_state['monolingual_xml_file_upload'])
Â  Â  else:
Â  Â  Â  Â  st.session_state['xml_structure_data'] = None
Â  Â Â 
Â  Â  return df_src


# ---------------------------------------------------------------------
# Parallel XML Loader
# ---------------------------------------------------------------------
@st.cache_data
def load_xml_parallel_corpus(src_file, tgt_file, src_lang_code, tgt_lang_code):
Â  Â  global SOURCE_LANG_CODE, TARGET_LANG_CODE

Â  Â  st.session_state['parallel_mode'] = False
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
Â  Â  st.session_state['target_sent_map'] = {}
Â  Â  st.session_state['monolingual_xml_file_upload'] = None # Clear mono XML state
Â  Â  st.session_state['xml_structure_data'] = None # Reset old structure
Â  Â  st.session_state['xml_structure_error'] = None # Reset old error
Â  Â Â 
Â  Â  if src_file is None or tgt_file is None: return None

Â  Â  # Reset file pointers before parsing
Â  Â  src_file.seek(0)
Â  Â  tgt_file.seek(0)
Â  Â Â 
Â  Â  src_result = parse_xml_content_to_df(src_file)
Â  Â  tgt_result = parse_xml_content_to_df(tgt_file)
Â  Â Â 
Â  Â  if src_result is None or tgt_result is None:
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  df_src = pd.DataFrame(src_result['df_data'])
Â  Â  df_tgt = pd.DataFrame(tgt_result['df_data'])

Â  Â  # 1. Check for Alignment (Sentence IDs)
Â  Â  src_sent_ids = set(df_src['sent_id'].unique())
Â  Â  tgt_sent_ids = set(df_tgt['sent_id'].unique())
Â  Â Â 
Â  Â  if src_sent_ids != tgt_sent_ids:
Â  Â  Â  Â  missing_in_tgt = src_sent_ids - tgt_sent_ids
Â  Â  Â  Â  missing_in_src = tgt_sent_ids - src_sent_ids
Â  Â  Â  Â Â 
Â  Â  Â  Â  error_msg = f"Alignment Check Failed: Sentence IDs mismatch."
Â  Â  Â  Â  if missing_in_tgt:
Â  Â  Â  Â  Â  Â  error_msg += f" Source ({src_result['lang_code']}) is missing sentence IDs: {sorted(list(missing_in_tgt))[:5]}..."
Â  Â  Â  Â  if missing_in_src:
Â  Â  Â  Â  Â  Â  error_msg += f" Target ({tgt_result['lang_code']}) is missing sentence IDs: {sorted(list(missing_in_src))[:5]}..."
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.error(error_msg)
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  # 2. Finalize Session State
Â  Â  # For Parallel mode, we use the codes provided in the text inputs
Â  Â  SOURCE_LANG_CODE = src_lang_codeÂ 
Â  Â  TARGET_LANG_CODE = tgt_lang_codeÂ 

Â  Â  df_src["_token_low"] = df_src["token"].str.lower()
Â  Â Â 
Â  Â  st.session_state['parallel_mode'] = True
Â  Â  st.session_state['df_target_lang'] = df_tgt
Â  Â  st.session_state['target_sent_map'] = tgt_result['sent_map']Â 
Â  Â Â 
Â  Â  # --- XML Structure Extraction for Overview (Combining structures) ---
Â  Â  # We rely on the structure extraction to use a fresh read/copy inside.
Â  Â  src_file.seek(0)
Â  Â  tgt_file.seek(0)
Â  Â  src_structure = extract_xml_structure(src_file)
Â  Â  tgt_file.seek(0)
Â  Â  tgt_structure = extract_xml_structure(tgt_file)
Â  Â Â 
Â  Â  combined_structure = {}
Â  Â  if src_structure:
Â  Â  Â  Â  combined_structure.update(src_structure)
Â  Â  if tgt_structure:
Â  Â  Â  Â  # Merge target structure, prioritizing source if tags clash, but merging attributes
Â  Â  Â  Â  for tag, attrs in tgt_structure.items():
Â  Â  Â  Â  Â  Â  if tag not in combined_structure:
Â  Â  Â  Â  Â  Â  Â  Â  combined_structure[tag] = attrs
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  for attr, values in attrs.items():
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if attr not in combined_structure[tag]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combined_structure[tag][attr] = values
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combined_structure[tag][attr].update(values)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Keep only 20 unique samples
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  combined_structure[tag][attr] = set(list(combined_structure[tag][attr])[:20])

Â  Â  st.session_state['xml_structure_data'] = combined_structure
Â  Â Â 
Â  Â  return df_src


# ---------------------------------------------------------------------
# EXISTING: Excel Parallel Corpus Loading
# ---------------------------------------------------------------------
@st.cache_data
def load_excel_parallel_corpus_file(file_source, excel_format):
Â  Â  global SOURCE_LANG_CODE, TARGET_LANG_CODE
Â  Â Â 
Â  Â  st.session_state['parallel_mode'] = False
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
Â  Â  st.session_state['target_sent_map'] = {}
Â  Â  st.session_state['monolingual_xml_file_upload'] = None # Clear mono XML state
Â  Â  st.session_state['xml_structure_data'] = None # Clear structure data
Â  Â  st.session_state['xml_structure_error'] = None # Clear structure error
Â  Â Â 
Â  Â  if file_source is None: return None
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  # Reset file pointer
Â  Â  Â  Â  file_source.seek(0)
Â  Â  Â  Â  df_raw = pd.read_excel(file_source, engine='openpyxl')
Â  Â  except Exception as e:
Â  Â  Â  Â  st.error(f"Failed to read Excel file: {e}")
Â  Â  Â  Â  return None

Â  Â  if df_raw.shape[1] < 2:
Â  Â  Â  Â  st.error("Excel file must contain at least two columns for source and target language.")
Â  Â  Â  Â  return None
Â  Â Â 
Â  Â  src_lang = df_raw.columns[0]
Â  Â  tgt_lang = df_raw.columns[1]
Â  Â Â 
Â  Â  # These set the global codes for Excel file headers
Â  Â  SOURCE_LANG_CODE = src_lang
Â  Â  TARGET_LANG_CODE = tgt_lang
Â  Â Â 
Â  Â  data_src = []
Â  Â  target_sent_map = {}
Â  Â  sent_id_counter = 0
Â  Â Â 
Â  Â  for index, row in df_raw.iterrows():
Â  Â  Â  Â  sent_id_counter += 1
Â  Â  Â  Â  src_text = str(row.iloc[0]).strip()
Â  Â  Â  Â  tgt_text = str(row.iloc[1]).strip()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- FIXED TOKENIZATION ---
Â  Â  Â  Â  cleaned_text = re.sub(r'([^\w\s])', r' \1 ', src_text)
Â  Â  Â  Â  src_tokens = [t.strip() for t in cleaned_text.split() if t.strip()]
Â  Â  Â  Â  # --------------------------
Â  Â  Â  Â Â 
Â  Â  Â  Â  target_sent_map[sent_id_counter] = tgt_textÂ 
Â  Â  Â  Â Â 
Â  Â  Â  Â  for token in src_tokens:
Â  Â  Â  Â  Â  Â  data_src.append({
Â  Â  Â  Â  Â  Â  Â  Â  "token": token,
Â  Â  Â  Â  Â  Â  Â  Â  "pos": "##",
Â  Â  Â  Â  Â  Â  Â  Â  "lemma": "##",
Â  Â  Â  Â  Â  Â  Â  Â  "sent_id": sent_id_counter
Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â  Â  Â Â 
Â  Â  if not data_src:
Â  Â  Â  Â  st.error("No valid sentences found in the parallel corpus.")
Â  Â  Â  Â  return None

Â  Â  df_src = pd.DataFrame(data_src)
Â  Â  df_src["_token_low"] = df_src["token"].str.lower()

Â  Â  st.session_state['parallel_mode'] = True
Â  Â  st.session_state['target_sent_map'] = target_sent_map
Â  Â Â 
Â  Â  # Handle XML within Excel format (if tagged) - placeholder as it's complex
Â  Â  if 'with XML' in excel_format:
Â  Â  Â  Â  st.info("Note: 'Excel with XML' format is currently treated as standard Excel parallel text for tokenization purposes.")
Â  Â  Â  Â Â 
Â  Â  return df_src


# --- Monolingual File Dispatcher (Updated for Built-in) ---
@st.cache_data
def load_corpus_file_built_in(file_source, corpus_name, explicit_lang_code):
Â  Â  # This is a specific loader for built-in text files (old logic simplified)
Â  Â  global SOURCE_LANG_CODE, TARGET_LANG_CODE
Â  Â Â 
Â  Â  st.session_state['parallel_mode'] = False
Â  Â  st.session_state['df_target_lang'] = pd.DataFrame()
Â  Â  st.session_state['target_sent_map'] = {}
Â  Â  st.session_state['monolingual_xml_file_upload'] = None
Â  Â  st.session_state['xml_structure_data'] = NoneÂ 
Â  Â  st.session_state['xml_structure_error'] = None # Reset old error
Â  Â Â 
Â  Â  # Set the global language code from the user's explicit selection initially
Â  Â  SOURCE_LANG_CODE = explicit_lang_code
Â  Â  TARGET_LANG_CODE = 'NA'
Â  Â  Â  Â Â 
Â  Â  if file_source is None: return None
Â  Â Â 
Â  Â  # Check if the corpus name/URL suggests XML (KOSLAT-ID uses XML)
Â  Â  is_xml_corpus_name = "xml" in BUILT_IN_CORPORA.get(corpus_name, "").lower() or "xml" in corpus_name.lower()

Â  Â  if is_xml_corpus_name:
Â  Â  Â  Â  # --- Handle Built-in XML Corpus ---
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  # We must use a copy of the stream for parsing and structure extraction
Â  Â  Â  Â  Â  Â  file_source.seek(0)
Â  Â  Â  Â  Â  Â  file_copy_for_parsing = BytesIO(file_source.read())
Â  Â  Â  Â  Â  Â  file_copy_for_parsing.seek(0)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  xml_result = parse_xml_content_to_df(file_copy_for_parsing) # Use the robust XML parser
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if xml_result:
Â  Â  Â  Â  Â  Â  Â  Â  df = pd.DataFrame(xml_result['df_data'])
Â  Â  Â  Â  Â  Â  Â  Â  df["_token_low"] = df["token"].str.lower()
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # If user explicitly selected 'OTHER', update SOURCE_LANG_CODE with the detected code
Â  Â  Â  Â  Â  Â  Â  Â  if explicit_lang_code == 'OTHER' and xml_result['lang_code'] not in ('XML', 'OTHER'):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  SOURCE_LANG_CODE = xml_result['lang_code']
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  # IMPORTANT: Extract structure from the copy before it's garbage collected
Â  Â  Â  Â  Â  Â  Â  Â  file_copy_for_parsing.seek(0)
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['xml_structure_data'] = extract_xml_structure(file_copy_for_parsing)Â 
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  return df
Â  Â  Â  Â  Â  Â  # If XML parsing fails, fall through to raw text processing as a last resort.
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.warning(f"Failed to parse built-in XML corpus '{corpus_name}': {e}. Falling back to raw text processing.")
Â  Â  Â  Â  Â  Â  # Clear file_source to re-read as raw text below
Â  Â  Â  Â  Â  Â  file_source.seek(0)Â 

Â  Â  # --- Prepare content string for non-XML or failed XML built-ins ---
Â  Â  try:
Â  Â  Â  Â  file_source.seek(0)
Â  Â  Â  Â  file_bytes = file_source.read()

Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  file_content_str = file_bytes.decode('utf-8')
Â  Â  Â  Â  Â  Â  file_content_str = re.sub(r'(\s+\n|\n\s+)', '\n', file_content_str)
Â  Â  Â  Â  except UnicodeDecodeError:
Â  Â  Â  Â  Â  Â  file_content_str = file_bytes.decode('utf-8', errors='ignore')
Â  Â  Â  Â Â 
Â  Â  Â  Â  clean_lines = [line for line in file_content_str.splitlines() if line and not line.strip().startswith('#')]
Â  Â  Â  Â  raw_text = "\n".join(clean_lines)
Â  Â  except Exception as e:Â 
Â  Â  Â  Â  st.error(f"Error reading built-in file content: {e}")
Â  Â  Â  Â  return None

Â  Â  df = pd.DataFrame()
Â  Â Â 
Â  Â  # --- Built-in T/P/L logic (for Europarl, etc.) ---
Â  Â  is_vertical_format = ("europarl" in corpus_name.lower()) or ("corpus-query-systems" in BUILT_IN_CORPORA.get(corpus_name, "").lower() and not is_xml_corpus_name)

Â  Â  if is_vertical_format:
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  file_buffer_for_pandas = StringIO(raw_text)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  df = pd.read_csv(
Â  Â  Â  Â  Â  Â  Â  Â  file_buffer_for_pandas,Â 
Â  Â  Â  Â  Â  Â  Â  Â  sep=r'\s+',Â 
Â  Â  Â  Â  Â  Â  Â  Â  header=None,Â 
Â  Â  Â  Â  Â  Â  Â  Â  engine="python",Â 
Â  Â  Â  Â  Â  Â  Â  Â  dtype=str,Â 
Â  Â  Â  Â  Â  Â  Â  Â  skipinitialspace=True,
Â  Â  Â  Â  Â  Â  Â  Â  usecols=[0, 1, 2],Â 
Â  Â  Â  Â  Â  Â  Â  Â  names=['token', 'pos', 'lemma']
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  df["token"] = df["token"].fillna("").astype(str).str.strip()Â 
Â  Â  Â  Â  Â  Â  df["pos"] = df["pos"].fillna("###").astype(str)
Â  Â  Â  Â  Â  Â  df["lemma"] = df["lemma"].fillna("###").astype(str)
Â  Â  Â  Â  Â  Â  df['sent_id'] = 0Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.warning(f"Failed to parse built-in corpus '{corpus_name}' as T/P/L: {e}. Falling back to raw tokenization.")
Â  Â  Â  Â  Â  Â  df = pd.DataFrame()Â 
Â  Â Â 
Â  Â  if df.empty:
Â  Â  Â  Â  # Final Raw Text ProcessingÂ 
Â  Â  Â  Â  cleaned_text = re.sub(r'([^\w\s])', r' \1 ', raw_text)
Â  Â  Â  Â  tokens = [t.strip() for t in cleaned_text.split() if t.strip()]Â 
Â  Â  Â  Â  df = pd.DataFrame({
Â  Â  Â  Â  Â  Â  "token": tokens,
Â  Â  Â  Â  Â  Â  "pos": ["##"] * len(tokens),
Â  Â  Â  Â  Â  Â  "lemma": ["##"] * len(tokens)
Â  Â  Â  Â  })
Â  Â  Â  Â  df['sent_id'] = 0Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  df["_token_low"] = df["token"].str.lower()
Â  Â  return dfÂ 

# -----------------------------------------------------
# Function to display KWIC examples for collocates (MODIFIED FOR STYLING)
# -----------------------------------------------------
def display_collocation_kwic_examples(df_corpus, node_word, top_collocates_df, window, limit_per_collocate=1, is_parallel_mode=False, target_sent_map=None, show_pos=False, show_lemma=False):
Â  Â  """
Â  Â  Generates and displays KWIC examples for a list of top collocates.
Â  Â  Displays up to KWIC_COLLOC_DISPLAY_LIMIT total examples.
Â  Â  """
Â  Â  if top_collocates_df.empty:
Â  Â  Â  Â  st.info("No collocates to display examples for.")
Â  Â  Â  Â  return

Â  Â  colloc_list = top_collocates_df.head(KWIC_COLLOC_DISPLAY_LIMIT)
Â  Â  collex_rows_total = []
Â  Â Â 
Â  Â  # Custom KWIC table style (Now includes flexible width for columns)
Â  Â  collocate_example_table_style = f"""
Â  Â  <style>
Â  Â  .collex-table-container-fixed {{
Â  Â  Â  Â  max-height: 400px; /* Fixed height for scrollable view */
Â  Â  Â  Â  overflow-y: auto;
Â  Â  Â  Â  margin-bottom: 1rem;
Â  Â  Â  Â  width: 100%;
Â  Â  }}
Â  Â  .collex-table-inner table {{Â 
Â  Â  Â  Â  width: 100%;Â 
Â  Â  Â  Â  table-layout: fixed; /* Fixed layout for proportional columns */
Â  Â  Â  Â  font-family: monospace;Â 
Â  Â  Â  Â  color: white;Â 
Â  Â  Â  Â  font-size: 0.9em;
Â  Â  }}
Â  Â  .collex-table-inner th {{ font-weight: bold; text-align: center; background-color: #383838; white-space: nowrap; }}
Â  Â Â 
Â  Â  /* Apply explicit proportional widths to Left, Node, Right, and optionally Translation */
Â  Â  .collex-table-inner td:nth-child(1) {{ width: 8%; text-align: left; font-weight: bold; border-right: 1px solid #444; white-space: nowrap; }} /* Collocate Column */
Â  Â  .collex-table-inner td:nth-child(2) {{ width: 35%; text-align: right; white-space: normal; vertical-align: top; padding: 5px 10px; }} /* Left Context */
Â  Â  .collex-table-inner td:nth-child(3) {{Â 
Â  Â  Â  Â  Â  width: 15%;Â 
Â  Â  Â  Â  Â  text-align: center;Â 
Â  Â  Â  Â  Â  font-weight: bold;Â 
Â  Â  Â  Â  Â  background-color: #f0f0f0;Â 
Â  Â  Â  Â  Â  color: black;Â 
Â  Â  Â  Â  Â  white-space: normal; vertical-align: top; padding: 5px 10px;
Â  Â  }} /* Node */
Â  Â  .collex-table-inner td:nth-child(4) {{ width: 35%; text-align: left; white-space: normal; vertical-align: top; padding: 5px 10px; }} /* Right Context */
Â  Â  .collex-table-inner td:nth-child(5) {{ text-align: left; color: #CCFFCC; width: 7%; font-family: sans-serif; font-size: 0.8em; white-space: normal; }} /* Translation Column (if present, takes remaining width) */

Â  Â  </style>
Â  Â  """
Â  Â  st.markdown(collocate_example_table_style, unsafe_allow_html=True)
Â  Â Â 
Â  Â Â 
Â  Â  with st.spinner(f"Generating concordance examples for top {len(colloc_list)} collocates..."):
Â  Â  Â  Â  for rank, (index, row) in enumerate(colloc_list.iterrows()):
Â  Â  Â  Â  Â  Â  collocate_word = row['Collocate']
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # KWIC returns (kwic_rows, total_matches, raw_target_input, literal_freq, sent_ids, breakdown_df)
Â  Â  Â  Â  Â  Â  kwic_rows, total_matches, _, _, sent_ids, _ = generate_kwic(
Â  Â  Â  Â  Â  Â  Â  Â  df_corpus, node_word, window, window,Â 
Â  Â  Â  Â  Â  Â  Â  Â  pattern_collocate_input=collocate_word,Â 
Â  Â  Â  Â  Â  Â  Â  Â  pattern_collocate_pos_input="",Â 
Â  Â  Â  Â  Â  Â  Â  Â  pattern_window=window, # Use collocation window for context
Â  Â  Â  Â  Â  Â  Â  Â  limit=limit_per_collocate, # Show 1 example max
Â  Â  Â  Â  Â  Â  Â  Â  is_parallel_mode=is_parallel_mode, # Pass parallel flag
Â  Â  Â  Â  Â  Â  Â  Â  show_pos=show_pos, # Pass display flags
Â  Â  Â  Â  Â  Â  Â  Â  show_lemma=show_lemma # Pass display flags
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if kwic_rows:
Â  Â  Â  Â  Â  Â  Â  Â  # Assuming limit=1, we only take the first row
Â  Â  Â  Â  Â  Â  Â  Â  kwic_row = kwic_rows[0]
Â  Â  Â  Â  Â  Â  Â  Â  sent_id = sent_ids[0]
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  translation = ""
Â  Â  Â  Â  Â  Â  Â  Â  if is_parallel_mode and sent_id is not None and target_sent_map:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  translation = target_sent_map.get(sent_id, "TRANSLATION N/A")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  collex_rows_total.append({
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Collocate": f"{rank+1}. {collocate_word}",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Left Context": kwic_row['Left'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Node": kwic_row['Node'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Right Context": kwic_row['Right'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Translation": translation if is_parallel_mode else None # New column
Â  Â  Â  Â  Â  Â  Â  Â  })
Â  Â  Â  Â Â 
Â  Â  if collex_rows_total:
Â  Â  Â  Â  collex_df = pd.DataFrame(collex_rows_total)
Â  Â  Â  Â  # Drop translation column if not in parallel mode
Â  Â  Â  Â  if not is_parallel_mode:
Â  Â  Â  Â  Â  Â  collex_df = collex_df.drop(columns=['Translation'])

Â  Â  Â  Â  # Manually create header for the collocate example table
Â  Â  Â  Â  header = "<tr><th>Collocate (Rank)</th><th>Left Context</th><th>Node</th><th>Right Context</th>"
Â  Â  Â  Â  if is_parallel_mode:
Â  Â  Â  Â  Â  Â  header += f"<th>Translation ({TARGET_LANG_CODE})</th>"
Â  Â  Â  Â  header += "</tr>"
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use HTML table and escape=False to preserve the HTML formatting (inline styles)
Â  Â  Â  Â  html_table = collex_df.to_html(header=False, escape=False, classes=['collex-table-inner'], index=False)
Â  Â  Â  Â  # Insert the custom header before the table body
Â  Â  Â  Â  html_table = html_table.replace("<thead></thead>", f"<thead>{header}</thead>", 1)
Â  Â  Â  Â Â 
Â  Â  Â  Â  scrollable_html = f"<div class='collex-table-container-fixed'>{html_table}</div>"
Â  Â  Â  Â  st.markdown(scrollable_html, unsafe_allow_html=True)
Â  Â  Â  Â  st.caption(f"Context window is set to **Â±{window} tokens** (Collocation window). Matching collocate is **bolded and highlighted bright yellow**. POS/Lemma display: **{show_pos}**/**{show_lemma}**.")
Â  Â  else:
Â  Â  Â  Â  st.info(f"No specific KWIC examples found for the top {len(colloc_list)} collocates within the Â±{window} window.")
# -----------------------------------------------------


# -----------------------------------------------------
# COLLOCATION LOGIC (omitted for brevity)
# -----------------------------------------------------
@st.cache_data(show_spinner=False)
def generate_collocation_results(df_corpus, raw_target_input, coll_window, mi_min_freq, max_collocates, is_raw_mode, collocate_regex="", collocate_pos_regex_input="", selected_pos_tags=None, collocate_lemma=""):
Â  Â  """
Â  Â  Generalized function to run collocation analysis.
Â  Â  Returns: (stats_df_sorted, freq, primary_target_mwu)
Â  Â  """
Â  Â Â 
Â  Â  total_tokens = len(df_corpus)
Â  Â  tokens_lower = df_corpus["_token_low"].tolist()
Â  Â Â 
Â  Â  # --- MWU/WILDCARD/STRUCTURAL RESOLUTION (reused from KWIC logic) ---
Â  Â  search_terms = raw_target_input.split()
Â  Â  primary_target_len = len(search_terms)
Â  Â Â 
Â  Â  def create_structural_matcher(term):
Â  Â  Â  Â  lemma_pattern = None; pos_pattern = None
Â  Â  Â  Â  lemma_match = re.search(r"\[(.*?)\]", term)
Â  Â  Â  Â  if lemma_match:
Â  Â  Â  Â  Â  Â  lemma_input = lemma_match.group(1).strip().lower()
Â  Â  Â  Â  Â  Â  if lemma_input: lemma_pattern = re.compile(f"^{re.escape(lemma_input).replace(r'\*', '.*')}$")
Â  Â  Â  Â  pos_match = re.search(r"\_([\w\*|]+)", term)
Â  Â  Â  Â  if pos_match:
Â  Â  Â  Â  Â  Â  pos_input = pos_match.group(1).strip()
Â  Â  Â  Â  Â  Â  if pos_input:
Â  Â  Â  Â  Â  Â  Â  Â  pos_patterns = [p.strip() for p in pos_input.split('|') if p.strip()]
Â  Â  Â  Â  Â  Â  Â  Â  pos_pattern = re.compile("^(" + "|".join([re.escape(p).replace(r'\*', '.*') for p in pos_patterns]) + ")$")
Â  Â  Â  Â  if lemma_pattern or pos_pattern: return {'type': 'structural', 'lemma_pattern': lemma_pattern, 'pos_pattern': pos_pattern}
Â  Â  Â  Â  pattern = re.escape(term.lower()).replace(r'\*', '.*')
Â  Â  Â  Â  return {'type': 'word', 'pattern': re.compile(f"^{pattern}$")}
Â  Â  Â  Â Â 
Â  Â  search_components = [create_structural_matcher(term) for term in search_terms]
Â  Â  all_target_positions = []
Â  Â Â 
Â  Â  # Execute Search Loop
Â  Â  if primary_target_len == 1 and not any('structural' == c['type'] for c in search_components):
Â  Â  Â  Â  target_pattern = search_components[0]['pattern']
Â  Â  Â  Â  for i, token in enumerate(tokens_lower):
Â  Â  Â  Â  Â  Â  if target_pattern.fullmatch(token):
Â  Â  Â  Â  Â  Â  Â  Â  all_target_positions.append(i)
Â  Â  else:
Â  Â  Â  Â  for i in range(len(tokens_lower) - primary_target_len + 1):
Â  Â  Â  Â  Â  Â  match = True
Â  Â  Â  Â  Â  Â  for k, component in enumerate(search_components):
Â  Â  Â  Â  Â  Â  Â  Â  corpus_index = i + k
Â  Â  Â  Â  Â  Â  Â  Â  if corpus_index >= len(df_corpus): break
Â  Â  Â  Â  Â  Â  Â  Â  if component['type'] == 'word':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not component['pattern'].fullmatch(tokens_lower[corpus_index]): match = False; break
Â  Â  Â  Â  Â  Â  Â  Â  elif component['type'] == 'structural':
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_lemma = df_corpus["lemma"].iloc[corpus_index].lower()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  current_pos = df_corpus["pos"].iloc[corpus_index]
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lemma_match = component['lemma_pattern'] is None or component['lemma_pattern'].fullmatch(current_lemma)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  pos_match = component['pos_pattern'] is None or component['pos_pattern'].fullmatch(current_pos)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not (lemma_match and pos_match): match = False; break
Â  Â  Â  Â  Â  Â  if match: all_target_positions.append(i)
Â  Â  Â  Â  Â  Â Â 
Â  Â  primary_target_positions = all_target_positionsÂ 
Â  Â  freq = len(primary_target_positions)
Â  Â  primary_target_mwu = raw_target_input

Â  Â  if freq == 0:
Â  Â  Â  Â  return (pd.DataFrame(), 0, raw_target_input)

Â  Â  # --- COLLOCATION COUNTING ---
Â  Â  collocate_directional_counts = Counter()Â 
Â  Â Â 
Â  Â  PUNCTUATION_COLLOCATES = PUNCTUATION # Defined globally
Â  Â Â 
Â  Â  for i in primary_target_positions:
Â  Â  Â  Â  start_index = max(0, i - coll_window)
Â  Â  Â  Â  end_index = min(total_tokens, i + primary_target_len + coll_window)Â 
Â  Â  Â  Â Â 
Â  Â  Â  Â  for j in range(start_index, end_index):
Â  Â  Â  Â  Â  Â  if i <= j < i + primary_target_len: continue
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  w = tokens_lower[j]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # FIX: Filter out punctuation collocates here
Â  Â  Â  Â  Â  Â  if w in PUNCTUATION_COLLOCATES or w.isdigit():
Â  Â  Â  Â  Â  Â  Â  Â  Â continue
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  p = df_corpus["pos"].iloc[j]
Â  Â  Â  Â  Â  Â  l = df_corpus["lemma"].iloc[j].lower() if "lemma" in df_corpus.columns else "##"
Â  Â  Â  Â  Â  Â  direction = 'L' if j < i else 'R'
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  collocate_directional_counts[(w, p, l, direction)] += 1
Â  Â Â 
Â  Â  raw_stats_data = {}Â 
Â  Â  token_counts_unfiltered = Counter(tokens_lower)Â 

Â  Â  for (w, p, l, direction), observed_dir in collocate_directional_counts.items():
Â  Â  Â  Â  key_tuple = (w, p, l)
Â  Â  Â  Â  if key_tuple not in raw_stats_data:
Â  Â  Â  Â  Â  Â  raw_stats_data[key_tuple] = {'L': 0, 'R': 0, 'Total': 0, 'w': w, 'p': p, 'l': l}
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  raw_stats_data[key_tuple][direction] += observed_dir
Â  Â  Â  Â  raw_stats_data[key_tuple]['Total'] += observed_dir

Â  Â  stats_list = []
Â  Â  for key_tuple, data in raw_stats_data.items():
Â  Â  Â  Â  w, p, l = key_tuple
Â  Â  Â  Â  observed = data['Total']
Â  Â  Â  Â  dominant_direction = 'R' if data['R'] > data['L'] else ('L' if data['L'] > data['R'] else 'B')
Â  Â  Â  Â  total_freq = token_counts_unfiltered.get(w, 0)
Â  Â  Â  Â Â 
Â  Â  Â  Â  k11 = observed
Â  Â  Â  Â  k12 = freq - k11
Â  Â  Â  Â  k21 = total_freq - k11
Â  Â  Â  Â  k22 = total_tokens - (k11 + k12 + k21)
Â  Â  Â  Â Â 
Â  Â  Â  Â  ll = compute_ll(k11, k12, k21, k22)
Â  Â  Â  Â  mi = compute_mi(k11, freq, total_freq, total_tokens)
Â  Â  Â  Â Â 
Â  Â  Â  Â  stats_list.append({
Â  Â  Â  Â  Â  Â  "Collocate": w, "POS": p, "Lemma": l, "Observed": observed,
Â  Â  Â  Â  Â  Â  "Total_Freq": total_freq, "LL": round(ll,6), "MI": round(mi,6),
Â  Â  Â  Â  Â  Â  "Significance": significance_from_ll(ll), "Direction": dominant_direction,Â 
Â  Â  Â  Â  Â  Â  "Obs_L": data['L'], "Obs_R": data['R']Â 
Â  Â  Â  Â  })

Â  Â  stats_df = pd.DataFrame(stats_list)
Â  Â Â 
Â  Â  # --- APPLY FILTERS ---
Â  Â  filtered_df = stats_df.copy()
Â  Â Â 
Â  Â  if collocate_regex:
Â  Â  Â  Â  pattern = re.escape(collocate_regex).replace(r'\*', '.*').replace(r'\|', '|').replace(r'\.', '.')
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  filtered_df = filtered_df[filtered_df['Collocate'].str.fullmatch(pattern, case=True, na=False)]
Â  Â  Â  Â  except re.error:
Â  Â  Â  Â  Â  Â  filtered_df = pd.DataFrame()Â 
Â  Â  Â  Â  Â  Â Â 
Â  Â  if collocate_pos_regex_input and not is_raw_mode:
Â  Â  Â  Â  pos_patterns = [p.strip() for p in collocate_pos_regex_input.split('|') if p.strip()]
Â  Â  Â  Â  full_pos_regex_list = [re.escape(p).replace(r'\*', '.*') for p in pos_patterns]
Â  Â  Â  Â  if full_pos_regex_list:
Â  Â  Â  Â  Â  Â  full_pos_regex = "^(" + "|".join(full_pos_regex_list) + ")$"
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  filtered_df = filtered_df[filtered_df['POS'].str.contains(full_pos_regex, case=True, na=False, regex=True)]
Â  Â  Â  Â  Â  Â  except re.error:
Â  Â  Â  Â  Â  Â  Â  Â  filtered_df = pd.DataFrame()
Â  Â  Â  Â Â 
Â  Â  if selected_pos_tags and not is_raw_mode and not collocate_pos_regex_input:
Â  Â  Â  Â  filtered_df = filtered_df[filtered_df['POS'].isin(selected_pos_tags)]
Â  Â  Â  Â Â 
Â  Â  if collocate_lemma and 'Lemma' in filtered_df.columns and not is_raw_mode:Â 
Â  Â  Â  Â  lemma_pattern = re.escape(collocate_lemma).replace(r'\*', '.*').replace(r'\|', '|').replace(r'\.', '.')
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  filtered_df = filtered_df[filtered_df['Lemma'].str.fullmatch(lemma_pattern, case=True, na=False)]
Â  Â  Â  Â  except re.error:
Â  Â  Â  Â  Â  Â  Â filtered_df = pd.DataFrame()
Â  Â Â 
Â  Â  stats_df_filtered = filtered_df
Â  Â Â 
Â  Â  if stats_df_filtered.empty:
Â  Â  Â  Â  return (pd.DataFrame(), freq, primary_target_mwu)
Â  Â  Â  Â Â 
Â  Â  stats_df_sorted = stats_df_filtered.sort_values("LL", ascending=False)
Â  Â Â 
Â  Â  return (stats_df_sorted, freq, primary_target_mwu)

# ---------------------------
# UI: header
# ---------------------------
st.title("CORTEX - Corpus Texts Explorer ")
st.caption("Upload vertical corpus (**token POS lemma**) or **raw horizontal text**, or **Parallel Corpus (Excel/XML)**.")

# ---------------------------
# Panel: upload and corpus info
# ---------------------------
corpus_source = None
corpus_name = "Uploaded File"
df_source_lang_for_analysis = None
parallel_uploaded = False

# --- SIDEBAR: CORPUS SELECTION, NAVIGATION, & MODULE SETTINGS ---
with st.sidebar:
Â  Â Â 
Â  Â  # 1. CORPUS SELECTION (TOP)
Â  Â  st.header("1. Corpus Source")
Â  Â Â 
Â  Â  # --- A. BUILT-IN SELECTION ---
Â  Â  st.markdown("##### ðŸ“¦ Built-in Corpus")
Â  Â  selected_corpus_name = st.selectbox(
Â  Â  Â  Â  "Select a pre-loaded corpus:",Â 
Â  Â  Â  Â  options=list(BUILT_IN_CORPORA.keys()),
Â  Â  Â  Â  key="corpus_select",Â 
Â  Â  Â  Â  on_change=reset_analysis
Â  Â  )
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- C. GLOBAL LANGUAGE SELECTION (NEW) ---
Â  Â  st.markdown("##### ðŸŒ Language Setting")
Â  Â Â 
Â  Â  # Set the initial index based on the session state's explicit code
Â  Â  initial_lang_index = 0
Â  Â  if st.session_state.get('user_explicit_lang_code') == 'ID':
Â  Â  Â  Â  initial_lang_index = 1
Â  Â  elif st.session_state.get('user_explicit_lang_code') == 'OTHER':
Â  Â  Â  Â  initial_lang_index = 2

Â  Â  selected_lang_name = st.selectbox(
Â  Â  Â  Â  "Corpus Language (Explicit Selection):",
Â  Â  Â  Â  options=["English (EN)", "Indonesian (ID)", "Other (RAW/XML Tag)"],
Â  Â  Â  Â  key="global_lang_select",
Â  Â  Â  Â  index=initial_lang_index,Â 
Â  Â  Â  Â  on_change=reset_analysis # Reset analysis completely when language changes
Â  Â  )
Â  Â Â 
Â  Â  # Map selected language to code and store explicitly
Â  Â  lang_code_map_sidebar = {"English (EN)": "EN", "Indonesian (ID)": "ID", "Other (RAW/XML Tag)": "OTHER"}
Â  Â  explicit_lang_code = lang_code_map_sidebar.get(selected_lang_name, 'OTHER')
Â  Â  st.session_state['user_explicit_lang_code'] = explicit_lang_code
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- B. CUSTOM CORPUS SELECTION MODE ---
Â  Â  corpus_mode = st.radio(
Â  Â  Â  Â  "Choose Corpus Type:",
Â  Â  Â  Â  options=["Monolingual Corpus", "Parallel Corpus"],
Â  Â  Â  Â  key="corpus_mode_radio",
Â  Â  Â  Â  on_change=reset_analysis
Â  Â  )

Â  Â  # --- B1. MONOLINGUAL CORPUS UPLOAD ---
Â  Â  if corpus_mode == "Monolingual Corpus":
Â  Â  Â  Â  st.markdown("##### ðŸ“ Monolingual File(s) Upload")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Format Selection (Language selection is now explicit)
Â  Â  Â  Â  selected_format_mono = st.selectbox(
Â  Â  Â  Â  Â  Â  "1. Choose Format:",
Â  Â  Â  Â  Â  Â  options=[
Â  Â  Â  Â  Â  Â  Â  Â  ".txt (Raw Text/Linear)",
Â  Â  Â  Â  Â  Â  Â  Â  ".xml (Raw Text/Linear)",
Â  Â  Â  Â  Â  Â  Â  Â  ".txt verticalised (T/P/L columns)",
Â  Â  Â  Â  Â  Â  Â  Â  ".xml verticalised (XML with <w> tags)",
Â  Â  Â  Â  Â  Â  Â  Â  ".txt TreeTagger format",
Â  Â  Â  Â  Â  Â  Â  Â  ".xml TreeTagger format"
Â  Â  Â  Â  Â  Â  ],
Â  Â  Â  Â  Â  Â  key="mono_format_select"
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # File Uploader (Allow multiple)
Â  Â  Â  Â  uploaded_files_mono = st.file_uploader(
Â  Â  Â  Â  Â  Â  "2. Upload Corpus File(s):",Â 
Â  Â  Â  Â  Â  Â  type=["txt","xml", "csv"],Â 
Â  Â  Â  Â  Â  Â  accept_multiple_files=True,
Â  Â  Â  Â  Â  Â  key="mono_file_upload",
Â  Â  Â  Â  Â  Â  on_change=reset_analysis
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Custom Monolingual Loading Logic
Â  Â  Â  Â  if uploaded_files_mono:
Â  Â  Â  Â  Â  Â  Â with st.spinner(f"Processing Monolingual Corpus ({len(uploaded_files_mono)} file(s))..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â df_source_lang_for_analysis = load_monolingual_corpus_files(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â uploaded_files_mono,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â explicit_lang_code, # Use the new explicit selection
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â selected_format_mono
Â  Â  Â  Â  Â  Â  Â  Â  Â )
Â  Â  Â  Â  Â  Â  Â  Â  Â if df_source_lang_for_analysis is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â corpus_name = f"Monolingual ({SOURCE_LANG_CODE}, {selected_format_mono})"
Â  Â Â 
Â  Â  # --- B2. PARALLEL CORPUS UPLOAD ---
Â  Â  else: # Parallel Corpus
Â  Â  Â  Â  st.markdown("##### ðŸ”— Parallel Corpus Upload")
Â  Â  Â  Â Â 
Â  Â  Â  Â  parallel_file_mode = st.radio(
Â  Â  Â  Â  Â  Â  "1. Choose File Structure:",
Â  Â  Â  Â  Â  Â  options=["One corpus file", "Two corpus files (aligned IDs required)"],
Â  Â  Â  Â  Â  Â  key="parallel_file_mode_radio"
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  if parallel_file_mode == "One corpus file":
Â  Â  Â  Â  Â  Â  excel_format = st.radio(
Â  Â  Â  Â  Â  Â  Â  Â  "2. Choose Format:",
Â  Â  Â  Â  Â  Â  Â  Â  options=[".xlsx (Col 1: Source, Col 2: Target)", ".xlsx with XML (Aligned Text/Tags)"],
Â  Â  Â  Â  Â  Â  Â  Â  key="excel_format_radio"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  parallel_excel_file = st.file_uploader(
Â  Â  Â  Â  Â  Â  Â  Â  "3. Upload Excel File:",Â 
Â  Â  Â  Â  Â  Â  Â  Â  type=["xlsx"],
Â  Â  Â  Â  Â  Â  Â  Â  key="parallel_excel_file_upload",
Â  Â  Â  Â  Â  Â  Â  Â  on_change=reset_analysis
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if parallel_excel_file is not None:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Processing Excel Parallel Corpus..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_source_lang_for_analysis = load_excel_parallel_corpus_file(parallel_excel_file, excel_format)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df_source_lang_for_analysis is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  corpus_name = f"Parallel (Excel) ({SOURCE_LANG_CODE}/{TARGET_LANG_CODE})"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parallel_uploaded = True

Â  Â  Â  Â  else: # Two corpus files
Â  Â  Â  Â  Â  Â  xml_format = st.radio(
Â  Â  Â  Â  Â  Â  Â  Â  "2. Choose Format:",
Â  Â  Â  Â  Â  Â  Â  Â  options=[".xml verticalised", ".xml TreeTagger format"],
Â  Â  Â  Â  Â  Â  Â  Â  key="xml_format_parallel_radio"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  src_lang_input = st.text_input("Source Language Code (e.g., EN)", value=st.session_state.get('src_lang_code', 'EN'), key='src_lang_code_input')
Â  Â  Â  Â  Â  Â  tgt_lang_input = st.text_input("Target Language Code (e.g., ID)", value=st.session_state.get('tgt_lang_code', 'ID'), key='tgt_lang_code_input')
Â  Â  Â  Â  Â  Â  st.session_state['src_lang_code'] = src_lang_input
Â  Â  Â  Â  Â  Â  st.session_state['tgt_lang_code'] = tgt_lang_input

Â  Â  Â  Â  Â  Â  xml_src_file = st.file_uploader(
Â  Â  Â  Â  Â  Â  Â  Â  f"3. Upload Source Language XML ({src_lang_input})",Â 
Â  Â  Â  Â  Â  Â  Â  Â  type=["xml"],
Â  Â  Â  Â  Â  Â  Â  Â  key="xml_src_file_upload",
Â  Â  Â  Â  Â  Â  Â  Â  on_change=reset_analysis
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  xml_tgt_file = st.file_uploader(
Â  Â  Â  Â  Â  Â  Â  Â  f"4. Upload Target Language XML ({tgt_lang_input})",Â 
Â  Â  Â  Â  Â  Â  Â  Â  type=["xml"],
Â  Â  Â  Â  Â  Â  Â  Â  key="xml_tgt_file_upload",
Â  Â  Â  Â  Â  Â  Â  Â  on_change=reset_analysis
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if xml_src_file is not None and xml_tgt_file is not None:
Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Processing XML Parallel Corpus..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_source_lang_for_analysis = load_xml_parallel_corpus(xml_src_file, xml_tgt_file, src_lang_input, tgt_lang_input)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df_source_lang_for_analysis is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  corpus_name = f"Parallel (XML) ({SOURCE_LANG_CODE}/{TARGET_LANG_CODE})"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  parallel_uploaded = True

Â  Â  # --- C. BUILT-IN FALLBACK (Only executes if no custom file was loaded) ---
Â  Â  if df_source_lang_for_analysis is None and selected_corpus_name != "Select built-in corpus...":
Â  Â  Â  Â  corpus_url = BUILT_IN_CORPORA[selected_corpus_name]Â 
Â  Â  Â  Â  # Check if we need to download/load the file
Â  Â  Â  Â  if 'initial_load_complete' not in st.session_state or st.session_state['initial_load_complete'] == False:
Â  Â  Â  Â  Â  Â  with st.spinner(f"Downloading {selected_corpus_name}..."):
Â  Â  Â  Â  Â  Â  Â  Â  corpus_source = download_file_to_bytesio(corpus_url)
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â corpus_source = download_file_to_bytesio(corpus_url)Â 
Â  Â  Â  Â  corpus_name = selected_corpus_name
Â  Â  Â  Â  df_source_lang_for_analysis = load_corpus_file_built_in(corpus_source, corpus_name, explicit_lang_code)
Â  Â Â 
Â  Â  # Use the loaded DF for the rest of the sidebar logic
Â  Â  df_sidebar = df_source_lang_for_analysis
Â  Â Â 
Â  Â  # Determine tagging mode safely for filter visibility
Â  Â  is_raw_mode_sidebar = True
Â  Â  if df_sidebar is not None and 'pos' in df_sidebar.columns and len(df_sidebar) > 0:
Â  Â  Â  Â  # Check if 99% or more of tags are the default "##" or "###"
Â  Â  Â  Â  count_of_raw_tags = df_sidebar['pos'].str.contains('##|###', na=False).sum()
Â  Â  Â  Â  is_raw_mode_sidebar = (count_of_raw_tags / len(df_sidebar)) > 0.99
Â  Â Â 
Â  Â  # 2. NAVIGATION
Â  Â  st.markdown("---")
Â  Â  st.subheader("2. Navigation (TOOLS)")
Â  Â Â 
Â  Â  is_active_o = st.session_state['view'] == 'overview'
Â  Â  st.button("ðŸ“– Overview", key='nav_overview', on_click=set_view, args=('overview',), use_container_width=True, type="primary" if is_active_o else "secondary")
Â  Â Â 
Â  Â  # Removed Corpus Structure Navigation Button
Â  Â Â 
Â  Â  is_active_d = st.session_state['view'] == 'dictionary'Â 
Â  Â  st.button("ðŸ“˜ Dictionary", key='nav_dictionary', on_click=set_view, args=('dictionary',), use_container_width=True, type="primary" if is_active_d else "secondary")
Â  Â Â 
Â  Â  is_active_c = st.session_state['view'] == 'concordance'
Â  Â  st.button("ðŸ“š Concordance", key='nav_concordance', on_click=set_view, args=('concordance',), use_container_width=True, type="primary" if is_active_c else "secondary")
Â  Â Â 
Â  Â  is_active_n = st.session_state['view'] == 'n_gram' # NEW N-GRAM BUTTON
Â  Â  st.button("ðŸ”¢ N-Gram", key='nav_n_gram', on_click=set_view, args=('n_gram',), use_container_width=True, type="primary" if is_active_n else "secondary")

Â  Â  is_active_l = st.session_state['view'] == 'collocation'
Â  Â  st.button("ðŸ”— Collocation", key='nav_collocation', on_click=set_view, args=('collocation',), use_container_width=True, type="primary" if is_active_l else "secondary")

Â  Â  # 3. TOOL SETTINGS (Conditional Block)
Â  Â  if st.session_state['view'] != 'overview':
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.subheader("3. Tool Settings")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- UNIVERSAL DISPLAY SETTINGS (NEW) ---
Â  Â  Â  Â  st.markdown("##### KWIC/Context Display")
Â  Â  Â  Â Â 
Â  Â  Â  Â  has_pos_lemma_data = not is_raw_mode_sidebar
Â  Â  Â  Â Â 
Â  Â  Â  Â  if not has_pos_lemma_data:
Â  Â  Â  Â  Â  Â  st.info("POS/Lemma display requires a tagged corpus.")
Â  Â  Â  Â  Â  Â  st.session_state['show_pos_tag'] = False
Â  Â  Â  Â  Â  Â  st.session_state['show_lemma'] = False
Â  Â  Â  Â Â 
Â  Â  Â  Â  show_pos_tag = st.checkbox(
Â  Â  Â  Â  Â  Â  "Show POS Tag",Â 
Â  Â  Â  Â  Â  Â  value=st.session_state.get('show_pos_tag', False),Â 
Â  Â  Â  Â  Â  Â  key='show_pos_tag_checkbox',Â 
Â  Â  Â  Â  Â  Â  disabled=not has_pos_lemma_data
Â  Â  Â  Â  )
Â  Â  Â  Â  st.session_state['show_pos_tag'] = show_pos_tag

Â  Â  Â  Â  show_lemma = st.checkbox(
Â  Â  Â  Â  Â  Â  "Show Lemma",Â 
Â  Â  Â  Â  Â  Â  value=st.session_state.get('show_lemma', False),Â 
Â  Â  Â  Â  Â  Â  key='show_lemma_checkbox',
Â  Â  Â  Â  Â  Â  disabled=not has_pos_lemma_data
Â  Â  Â  Â  )
Â  Â  Â  Â  st.session_state['show_lemma'] = show_lemma
Â  Â  Â  Â Â 
Â  Â  Â  Â  if show_pos_tag or show_lemma:
Â  Â  Â  Â  Â  Â  st.caption("Context displays in the format: **token/TAG{lemma}**")

Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- CONCORDANCE SETTINGS ---
Â  Â  Â  Â  if st.session_state['view'] == 'concordance':
Â  Â  Â  Â  Â  Â  st.subheader("Concordance Parameters")
Â  Â  Â  Â  Â  Â  st.write("KWIC Context (Display)")
Â  Â  Â  Â  Â  Â  kwic_left = st.number_input("Left Context (tokens)", min_value=1, max_value=20, value=st.session_state.get('kwic_left', 7), step=1, help="Number of tokens shown to the left of the node word.", key="concordance_kwic_left")
Â  Â  Â  Â  Â  Â  kwic_right = st.number_input("Right Context (tokens)", min_value=1, max_value=20, value=st.session_state.get('kwic_right', 7), step=1, help="Number of tokens shown to the right of the node word.", key="concordance_kwic_right")
Â  Â  Â  Â  Â  Â  st.session_state['kwic_left'] = kwic_left
Â  Â  Â  Â  Â  Â  st.session_state['kwic_right'] = kwic_right
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.subheader("Pattern Search Filter")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.caption("The **Node Word** is set by the primary search input above.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  pattern_search_window = st.number_input(
Â  Â  Â  Â  Â  Â  Â  Â  "Search Window (tokens, each side)",Â 
Â  Â  Â  Â  Â  Â  Â  Â  min_value=1, max_value=10, value=st.session_state.get('pattern_search_window', 5), step=1,Â 
Â  Â  Â  Â  Â  Â  Â  Â  key="pattern_search_window_input",Â 
Â  Â  Â  Â  Â  Â  Â  Â  on_change=trigger_analysis_callbackÂ 
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  pattern_collocate = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  "Collocate Word/Pattern (* for wildcard)",Â 
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('pattern_collocate_input', ''),
Â  Â  Â  Â  Â  Â  Â  Â  key="pattern_collocate_input",Â 
Â  Â  Â  Â  Â  Â  Â  Â  on_change=trigger_analysis_callbackÂ 
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if df_sidebar is not None and 'pos' in df_sidebar.columns and not is_raw_mode_sidebar:
Â  Â  Â  Â  Â  Â  Â  Â  pattern_collocate_pos_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Collocate POS Tag Pattern (Wildcard/Concatenation)",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('pattern_collocate_pos_input', ''),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="pattern_collocate_pos_input",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="E.g., V* (Verbs), *G (Gerunds), NNS|NNP (Plural/Proper Nouns). Filters collocates by POS tag.",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  on_change=trigger_analysis_callbackÂ 
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['pattern_collocate_pos'] = pattern_collocate_pos_input
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("POS filtering for collocates requires a tagged corpus.")
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['pattern_collocate_pos'] = ''

Â  Â  Â  Â  Â  Â  st.session_state['pattern_search_window'] = pattern_search_window
Â  Â  Â  Â  Â  Â  st.session_state['pattern_collocate'] = pattern_collocate
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- N-GRAM SETTINGS ---
Â  Â  Â  Â  elif st.session_state['view'] == 'n_gram':
Â  Â  Â  Â  Â  Â  st.subheader("N-Gram Parameters")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # N-Gram size slider
Â  Â  Â  Â  Â  Â  n_gram_size = st.slider(
Â  Â  Â  Â  Â  Â  Â  Â  "N-Gram Size (N)",Â 
Â  Â  Â  Â  Â  Â  Â  Â  min_value=1, max_value=5,Â 
Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('n_gram_size', 2),Â 
Â  Â  Â  Â  Â  Â  Â  Â  step=1,Â 
Â  Â  Â  Â  Â  Â  Â  Â  key="n_gram_size_slider",
Â  Â  Â  Â  Â  Â  Â  Â  on_change=trigger_n_gram_analysis_callback,
Â  Â  Â  Â  Â  Â  Â  Â  help="Select the size of the token sequence (unigram, bigram, trigram, etc.)"
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.session_state['n_gram_size'] = n_gram_size
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.subheader("Positional Filters")

Â  Â  Â  Â  Â  Â  help_text = "Enter a pattern for filtering this position (wildcard `*` supported):\n\n"
Â  Â  Â  Â  Â  Â  help_text += "1. **Token/Word:** `govern*` or `the` (default regex match).\n"
Â  Â  Â  Â  Â  Â  if not is_raw_mode_sidebar:
Â  Â  Â  Â  Â  Â  Â  Â  help_text += "2. **POS Tag:** `_N*` (matches all tags starting with N).\n"
Â  Â  Â  Â  Â  Â  Â  Â  help_text += "3. **Lemma:** `[have]` (matches 'have', 'has', 'having', etc.)."
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("âš ï¸ Tagged corpus required for Lemma/POS filtering.")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Dynamic Positional Filter Boxes
Â  Â  Â  Â  Â  Â  current_n_gram_filters = st.session_state.get('n_gram_filters', {})
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Ensure filter list is correctly sized (up to N)
Â  Â  Â  Â  Â  Â  new_filters = {}
Â  Â  Â  Â  Â  Â  for i in range(1, n_gram_size + 1):
Â  Â  Â  Â  Â  Â  Â  Â  default_val = current_n_gram_filters.get(str(i), '')
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  input_key = f"n_gram_filter_{i}"
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  filter_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"Position {i} Filter",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=default_val,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key=input_key,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  on_change=trigger_n_gram_analysis_callback,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  args=(),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help=help_text
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  new_filters[str(i)] = filter_input.strip()

Â  Â  Â  Â  Â  Â  # Update session state filters, keeping only up to N-gram size
Â  Â  Â  Â  Â  Â  st.session_state['n_gram_filters'] = {k: v for k, v in new_filters.items() if int(k) <= n_gram_size}
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- COLLOCATION SETTINGS ---
Â  Â  Â  Â  elif st.session_state['view'] == 'collocation':
Â  Â  Â  Â  Â  Â  st.subheader("Collocation Parameters")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  max_collocates = st.number_input("Max Collocates to Show (Network/Tables)", min_value=5, max_value=100, value=st.session_state.get('max_collocates', 20), step=5, help="Maximum number of collocates displayed.", key="coll_max_collocates")
Â  Â  Â  Â  Â  Â  coll_window = st.number_input("Collocation window (tokens each side)", min_value=1, max_value=10, value=st.session_state.get('coll_window', 5), step=1, help="Window used for collocation counting (default Â±5).", key="coll_window_input")
Â  Â  Â  Â  Â  Â  mi_min_freq = st.number_input("MI minimum observed freq", min_value=1, max_value=100, value=st.session_state.get('mi_min_freq', 1), step=1, key="coll_mi_min_freq")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.session_state['max_collocates'] = max_collocates
Â  Â  Â  Â  Â  Â  st.session_state['coll_window'] = coll_window
Â  Â  Â  Â  Â  Â  st.session_state['mi_min_freq'] = mi_min_freq

Â  Â  Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  Â  Â  st.subheader("Collocate Filters")
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  collocate_regex = st.text_input("Filter by Word/Regex (* for wildcard)", value=st.session_state.get('collocate_regex_input', ''), key="collocate_regex_input_coll")
Â  Â  Â  Â  Â  Â  st.session_state['collocate_regex'] = collocate_regex
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if df_sidebar is not None and 'pos' in df_sidebar.columns and not is_raw_mode_sidebar:
Â  Â  Â  Â  Â  Â  Â  Â  collocate_pos_regex_input = st.text_input(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "Filter by POS Tag Pattern (Wildcard/Concatenation)",Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=st.session_state.get('collocate_pos_regex_input_coll', ''),Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="collocate_pos_regex_input_coll_tag",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="E.g., V* (Verbs), NNS|NNP (Plural/Proper Nouns)."
Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['collocate_pos_regex'] = collocate_pos_regex_input
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  all_pos_tags = sorted([tag for tag in df_sidebar['pos'].unique() if tag != '##' and tag != '###'])
Â  Â  Â  Â  Â  Â  Â  Â  if all_pos_tags:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  selected_pos_tags = st.multiselect(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  "OR Filter by specific POS Tag(s)",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  options=all_pos_tags,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  default=st.session_state.get('selected_pos_tags_input', None),
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="selected_pos_tags_input",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  help="Only shows collocates matching one of the selected POS tags. Ignored if Pattern is also set."
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['selected_pos_tags'] = selected_pos_tags
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("POS filtering requires a tagged corpus.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['collocate_pos_regex'] = ''
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['selected_pos_tags'] = None

Â  Â  Â  Â  Â  Â  if df_sidebar is not None and 'lemma' in df_sidebar.columns and not is_raw_mode_sidebar:
Â  Â  Â  Â  Â  Â  Â  Â  collocate_lemma_input = st.text_input("Filter by Lemma (case-insensitive, * for wildcard)", value=st.session_state.get('collocate_lemma_input', ''), key="collocate_lemma_input_coll")
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['collocate_lemma'] = collocate_lemma_input
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("Lemma filtering requires a lemmatized corpus.")
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['collocate_lemma'] = ''
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- DICTIONARY SETTINGS (Placeholder) ---
Â  Â  Â  Â  elif st.session_state['view'] == 'dictionary':
Â  Â  Â  Â  Â  Â  st.info("Dictionary module currently uses global Collocation Window/Filter settings for collocation analysis, accessible in the Collocation view.")

Â  Â  # --- 4. FEATURE STATUS CHECK (NEW) ---
Â  Â  st.markdown("---")
Â  Â  st.subheader("4. Feature Status")

Â  Â  # CEFR Status Check
Â  Â  cefr_status = f"**CEFR Categorization:** {'âœ… ACTIVE' if CEFR_FEATURE_AVAILABLE else 'âŒ DISABLED (Install `cefrpy`)'}"
Â  Â  if explicit_lang_code != 'EN' and CEFR_FEATURE_AVAILABLE:
Â  Â  Â  Â  cefr_status += f" (Disabled for **{explicit_lang_code}** corpus)"
Â  Â  st.markdown(cefr_status)

Â  Â  # IPA Status Check
Â  Â  ipa_status = f"**IPA Transcription:** {'âœ… ACTIVE' if IPA_FEATURE_AVAILABLE else 'âŒ DISABLED (Install `eng-to-ipa`)'}"
Â  Â  if explicit_lang_code != 'EN' and IPA_FEATURE_AVAILABLE:
Â  Â  Â  Â  ipa_status += f" (Disabled for non-English corpus)"
Â  Â  st.markdown(ipa_status)
# --- END SIDEBAR ---

# --- NEW ADDITION: AUTHOR INFO AND DOCUMENTATION LINK ---
Â  Â  st.markdown("---")
Â  Â  st.subheader("App Info")
Â  Â  st.markdown(
Â  Â  Â  Â  "This app is written by **Prihantoro** "
Â  Â  Â  Â  "([prihantoro@live.undip.ac.id](mailto:prihantoro@live.undip.ac.id); [www.prihantoro.com](http://www.prihantoro.com))"
Â  Â  )
Â  Â  # Add the clickable book icon and link
Â  Â  st.markdown(
Â  Â  Â  Â  """
Â  Â  Â  Â  <a href="https://docs.google.com/document/d/1rqrj3X_uoKWL_5P2QBlSQMW06R3EoknxqmpIcxTRrKI/edit?usp=sharing" target="_blank" style="text-decoration: none;">
Â  Â  Â  Â  Â  Â  <button style="background-color: #333333; color: white; border: none; padding: 10px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; margin: 4px 2px; cursor: pointer; border-radius: 4px;">
Â  Â  Â  Â  Â  Â  Â  Â  <span style="font-size: 1.2em;">ðŸ“–</span> App Documentation
Â  Â  Â  Â  Â  Â  </button>
Â  Â  Â  Â  </a>
Â  Â  Â  Â  """,Â 
Â  Â  Â  Â  unsafe_allow_html=True
Â  Â  )
# -----------------------------------------------------

# load corpus (cached) for main body access - Use the result from the sidebar
df = df_source_lang_for_analysis

# --- Check for initial load failure and display better message ---
if df is None:
Â  Â  st.header("ðŸ‘‹ Welcome to CORTEX!")
Â  Â  st.markdown("---")
Â  Â  st.markdown("## Get Started")
Â  Â  st.markdown("**Choose a preloaded corpus or upload your own corpus** in the sidebar to begin analysis.")
Â  Â  st.error(f"âŒ **CORPUS LOAD FAILED** or **NO CORPUS SELECTED**. Please check the sidebar selection and ensure files are correctly formatted/aligned.")
Â  Â  st.stop()
# ---------------------------------------------------------------------

# --- Define Language Suffix for Headers ---
is_parallel_mode_active = st.session_state.get('parallel_mode', False)

# --- Language Code Override for Monolingual Mode (v17.42 FIX) ---
if not is_parallel_mode_active:
Â  Â  Â # Force global code to match explicit user selection state for dictionary logic
Â  Â  Â # This is CRITICAL to ensure the dictionary module uses the correct language features
Â  Â  Â SOURCE_LANG_CODE = st.session_state.get('user_explicit_lang_code', 'EN')
# --------------------------------------------------------------------

lang_display_suffix = f" in **{SOURCE_LANG_CODE}**" if is_parallel_mode_active else ""
lang_input_suffix = f" in **{SOURCE_LANG_CODE}**" if is_parallel_mode_active else ""
is_monolingual_xml_loaded = st.session_state.get('monolingual_xml_file_upload') is not None
if st.session_state.get('xml_structure_data', None) is not None and not is_parallel_mode_active:
Â  Â  Â lang_display_suffix = f" in **{SOURCE_LANG_CODE} (XML Monolingual)**"
Â  Â  Â lang_input_suffix = f" in **{SOURCE_LANG_CODE} (XML Monolingual)**"
# ---------------------------------------------------------------


# --- CRITICAL STATUS MESSAGE FOR DEBUGGING (SUCCESS PATH) ---
is_raw_mode = True
if 'pos' in df.columns and len(df) > 0:
Â  Â  # Check if 99% or more of tags are the default "##" or "###"
Â  Â  count_of_raw_tags = df['pos'].str.contains('##|###', na=False).sum()
Â  Â  is_raw_mode = (count_of_raw_tags / len(df)) > 0.99
Â  Â Â 
if is_parallel_mode_active:
Â  Â  app_mode = f"Analyzing Parallel Corpus: {corpus_name} (Source: {SOURCE_LANG_CODE})"
Â  Â  st.info(f"âœ… Parallel Corpus loaded successfully. Total tokens ({SOURCE_LANG_CODE}): **{len(df):,}**. Total sentences: **{len(st.session_state['target_sent_map']):,}**.")
elif st.session_state.get('xml_structure_data', None) is not None and not is_parallel_mode_active:
Â  Â  Â # This case handles both uploaded XML and built-in XML (like KOSLAT-ID)
Â  Â  app_mode = f"Analyzing Corpus: {corpus_name} (XML Monolingual - {SOURCE_LANG_CODE})"
Â  Â  st.info(f"âœ… Monolingual XML Corpus **'{corpus_name}'** loaded successfully. Total tokens: **{len(df):,}**.")
else:
Â  Â  app_mode = f"Analyzing Corpus: {corpus_name} ({'RAW/LINEAR MODE' if is_raw_mode else 'TAGGED MODE'})"
Â  Â  st.info(f"âœ… Corpus **'{corpus_name}'** loaded successfully. Total tokens: **{len(df):,}**.")

st.markdown("---")
Â  Â Â 
# --- CORPUS STATS CALCULATION (SHARED) ---
total_tokens = len(df)
tokens_lower = df["_token_low"].tolist()
tokens_lower_filtered = [t for t in tokens_lower if t not in PUNCTUATION and not t.isdigit()]
token_counts = Counter(tokens_lower)Â 
unique_types = len(set(tokens_lower_filtered))
unique_lemmas = df["lemma"].nunique() if "lemma" in df.columns else "###"

freq_df_filtered = df[~df['_token_low'].isin(PUNCTUATION) & ~df['_token_low'].str.isdigit()].copy()
# Only include POS in the frequency table if it's a tagged corpus
if not is_raw_mode:
Â  Â  freq_df = freq_df_filtered[freq_df_filtered['token'] != ''].groupby(["token","pos"]).size().reset_index(name="frequency").sort_values("frequency", ascending=False).reset_index(drop=True)
else:
Â  Â  Â freq_df = freq_df_filtered[freq_df_filtered['token'] != ''].groupby(["token"]).size().reset_index(name="frequency").sort_values("frequency", ascending=False).reset_index(drop=True)
Â  Â  Â 
# -------------------------------------------------------------------------------------------------------


st.header(app_mode)

# -----------------------------------------------------
# MODULE: CORPUS OVERVIEW
# -----------------------------------------------------
if st.session_state['view'] == 'overview':
Â  Â Â 
Â  Â  col1, col2 = st.columns([2,1])
Â  Â  with col1:
Â  Â  Â  Â  st.subheader("Corpus Summary")
Â  Â  Â  Â  # STTR calculation omitted for brevity but can be easily added back
Â  Â  Â  Â  info_data = {
Â  Â  Â  Â  Â  Â  "Metric": [f"Corpus size ({SOURCE_LANG_CODE} tokens)", "Unique types (w/o punc)", "Lemma count"],
Â  Â  Â  Â  Â  Â  "Value": [f"{total_tokens:,}", unique_types, unique_lemmas]
Â  Â  Â  Â  }
Â  Â  Â  Â  if st.session_state.get('parallel_mode', False):
Â  Â  Â  Â  Â  Â  info_data["Metric"].append("Aligned Sentences")
Â  Â  Â  Â  Â  Â  info_data["Value"].append(f"{len(st.session_state['target_sent_map']):,}")

Â  Â  Â  Â  info_df = pd.DataFrame(info_data)
Â  Â  Â  Â  st.dataframe(info_df, use_container_width=True, hide_index=True)Â 

Â  Â  Â  Â  st.subheader("Word Cloud (Top Words - Stopwords Filtered)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- Word Cloud Display logic ---
Â  Â  Â  Â  if not freq_df.empty:
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if not WORDCLOUD_FEATURE_AVAILABLE:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("âš ï¸ **Word Cloud Feature Disabled:** Visualization requires the external `wordcloud` library, which could not be initialized. Please ensure it is installed correctly in your local environment.")
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  wordcloud_fig = create_word_cloud(freq_df, not is_raw_mode)
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  if wordcloud_fig is not None:Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not is_raw_mode:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  **Word Cloud Color Key (POS):** | <span style="color:#33CC33;">**Green**</span> Noun | <span style="color:#3366FF;">**Blue**</span> Verb | <span style="color:#FF33B5;">**Pink**</span> Adjective | <span style="color:#FFCC00;">**Yellow**</span> Adverb |
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  , unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(wordcloud_fig)
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info("Not enough single tokens remaining to generate a word cloud.")

Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.info("No tokens to generate a word cloud.")
Â  Â  Â  Â  # ---------------------------------------------------------------------------------

Â  Â  with col2:
Â  Â  Â  Â  st.subheader("Top frequency")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- MODIFICATION START ---
Â  Â  Â  Â  if not freq_df.empty:
Â  Â  Â  Â  Â  Â  freq_head = freq_df.head(10).copy()
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Calculate Relative Frequency (per M) for the Overview table
Â  Â  Â  Â  Â  Â  total_tokens_float = float(total_tokens)
Â  Â  Â  Â  Â  Â  freq_head['Relative Frequency (per M)'] = freq_head['frequency'].apply(lambda f: round((f / total_tokens_float) * 1_000_000, 4))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  # Reorder columns for display
Â  Â  Â  Â  Â  Â  display_cols = ["token", "frequency", "Relative Frequency (per M)"]
Â  Â  Â  Â  Â  Â  if 'pos' in freq_head.columns:
Â  Â  Â  Â  Â  Â  Â  Â  display_cols.insert(1, 'pos')
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  freq_head = freq_head[display_cols]
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  freq_head.insert(0,"No", range(1, len(freq_head)+1))
Â  Â  Â  Â  Â  Â  freq_head.rename(columns={'token': 'Token', 'frequency': 'Abs. Freq'}, inplace=True)
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  st.dataframe(freq_head, use_container_width=True, hide_index=True)Â 
Â  Â  Â  Â  Â  Â  st.download_button("â¬‡ Download full frequency list (xlsx)", data=df_to_excel_bytes(freq_df), file_name="full_frequency_list_filtered.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â st.info("Frequency data not available.")
Â  Â  Â  Â  # --- MODIFICATION END ---
Â  Â  Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- XML CORPUS STRUCTURE DISPLAY (NEW HIERARCHICAL DISPLAY) ---
Â  Â  structure_data = st.session_state.get('xml_structure_data')
Â  Â  structure_error = st.session_state.get('xml_structure_error')

Â  Â  # Display the section only if an XML file was involved (either monoline XML or parallel mode)
Â  Â  if is_monolingual_xml_loaded or is_parallel_mode_active or selected_corpus_name == "KOSLAT-ID (XML Tagged)":
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use a large, always visible expander for structure details
Â  Â  Â  Â  with st.expander("ðŸ“Š XML Corpus Structure (Details)", expanded=True):
Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if structure_error:
Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ **XML Parsing Failed in Parser Function.** The underlying Python `xml.etree.ElementTree.fromstring()` function raised the following error: \n\n`{structure_error}`")
Â  Â  Â  Â  Â  Â  Â  Â  st.info("This usually indicates severe malformation, an illegal XML character, or a missing closing tag in the raw corpus file.")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  if structure_data:
Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Structure and Attributes (Hierarchical View)")
Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  file_label = f"Source ({SOURCE_LANG_CODE})" if is_parallel_mode_active else f"Monolingual ({SOURCE_LANG_CODE})"
Â  Â  Â  Â  Â  Â  Â  Â  if is_parallel_mode_active:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.caption(f"Showing combined structure from **{SOURCE_LANG_CODE}** and **{TARGET_LANG_CODE}**.")
Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.caption(f"Showing structure from **{file_label}**. Attributes are sampled up to 20 unique values.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Use the hierarchical function to generate HTML
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  structure_html = format_structure_data_hierarchical(structure_data)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  f"""
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  <div style="font-family: monospace; font-size: 0.9em; padding: 10px; background-color: #282828; border-radius: 5px;">
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  {structure_html}
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  </div>
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  """,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  unsafe_allow_html=True
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # AGGRESSIVE DEBUGGING - Show error message if rendering fails (not parsing)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.error(f"âŒ **XML Hierarchical Display FAILED (Rendering Error: {e})**. Showing raw data structure below for diagnosis.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- DIAGNOSTIC/FALLBACK RAW TEXT DISPLAY ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.expander("Show Raw Python Data (for diagnosis)"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.info("The data below is the Python dictionary successfully produced by the XML parser.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Show the raw Python dictionary object
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.json(structure_data)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Fallback 2: Show the unstyled raw text output if json fails or for comparison
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â def format_structure_data_raw_text(structure_data, max_values=20):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lines = []
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for tag in sorted(structure_data.keys()):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lines.append(f"\n<{tag}>")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  for attr in sorted(structure_data[tag].keys()):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  values = sorted(list(structure_data[tag][attr]))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sampled_values_str = ", ".join(values[:max_values])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if len(values) > max_values:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sampled_values_str += f", ... ({len(values) - max_values} more unique)"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  lines.append(f"Â  Â  @{attr}: [{sampled_values_str}]")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "\n".join(lines)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.code(format_structure_data_raw_text(structure_data))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # --- END DIAGNOSTIC/FALLBACK ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  elif not structure_error:
Â  Â  Â  Â  Â  Â  Â  Â  # Only show this if no error occurred AND no data was returned (i.e., parser ran but found nothing)
Â  Â  Â  Â  Â  Â  Â  Â  st.info("XML structure not found in the loaded corpus. The corpus must be an XML file and well-formed.")
Â  Â  Â  Â  Â  Â  Â 
Â  Â  # -----------------------------------------------------

Â  Â  st.markdown("---")

# -----------------------------------------------------
# MODULE: SEARCH INPUT (SHARED FOR CONCORDANCE/COLLOCATION)
# -----------------------------------------------------

if st.session_state['view'] != 'overview' and st.session_state['view'] != 'dictionary' and st.session_state['view'] != 'n_gram':
Â  Â Â 
Â  Â  # --- SEARCH INPUT (SHARED) ---
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"Search Input: {st.session_state['view'].capitalize()}{lang_display_suffix}")
Â  Â Â 
Â  Â  # The input field that controls analysis for Concordance/Collocation
Â  Â  # FIX: Use conditional language suffix
Â  Â  typed_target = st.text_input(
Â  Â  Â  Â  f"Type a primary token/MWU (word* or 'in the') or Structural Query ([lemma*]_POS*){lang_input_suffix}",Â 
Â  Â  Â  Â  value=st.session_state.get('typed_target_input', ''),Â 
Â  Â  Â  Â  key="typed_target_input",
Â  Â  Â  Â  on_change=trigger_analysis_callback # Triggers analysis on Enter/change
Â  Â  )
Â  Â Â 
Â  Â  primary_input = typed_target.strip()
Â  Â  target_input = primary_input
Â  Â Â 
Â  Â  use_pattern_search = False
Â  Â  if st.session_state['view'] == 'concordance':
Â  Â  Â  Â  if primary_input and (st.session_state.get('pattern_collocate_input', '').strip() or st.session_state.get('pattern_collocate_pos_input', '').strip()):
Â  Â  Â  Â  Â  Â  use_pattern_search = True

Â  Â  if not target_input and not use_pattern_search and st.session_state['view'] not in ('dictionary', 'n_gram'):
Â  Â  Â  Â  st.info(f"Type a term or pattern for {st.session_state['view'].capitalize()} analysis.")
Â  Â Â 
Â  Â  # The explicit button for manual initiation
Â  Â  analyze_btn_explicit = st.button("ðŸ”Ž Analyze")
Â  Â Â 
Â  Â  analyze_btn = analyze_btn_explicit or st.session_state['trigger_analyze']
Â  Â  st.session_state['analyze_btn'] = analyze_btn # Store for downstream check
Â  Â  st.session_state['trigger_analyze'] = False
Â  Â Â 
Â  Â  st.markdown("---")


# -----------------------------------------------------
# MODULE: N-GRAM LOGIC
# -----------------------------------------------------
if st.session_state['view'] == 'n_gram':
Â  Â Â 
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"ðŸ”¢ N-Gram Frequency Analysis (N={st.session_state['n_gram_size']}){lang_display_suffix}")
Â  Â Â 
Â  Â  # Check if a rerun was triggered by changing a filter/size, OR if the analysis was reset due to corpus change.
Â  Â  analyze_n_gram = st.session_state['n_gram_trigger_analyze'] or st.session_state['n_gram_results_df'].empty
Â  Â  st.session_state['n_gram_trigger_analyze'] = False # Reset the trigger immediately
Â  Â Â 
Â  Â  # Force re-analysis if manual button is pressed
Â  Â  manual_analyze_btn = st.button("ðŸ”Ž Re-Analyze N-Grams")
Â  Â  if manual_analyze_btn:
Â  Â  Â  Â  analyze_n_gram = True
Â  Â Â 
Â  Â  if analyze_n_gram:
Â  Â  Â  Â  with st.spinner(f"Generating and filtering {st.session_state['n_gram_size']}-grams..."):
Â  Â  Â  Â  Â  Â  # FIX: Passed corpus_name as a unique ID to break the cache when corpus changes
Â  Â  Â  Â  Â  Â  n_gram_df = generate_n_grams(
Â  Â  Â  Â  Â  Â  Â  Â  df,Â 
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['n_gram_size'],
Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['n_gram_filters'],
Â  Â  Â  Â  Â  Â  Â  Â  is_raw_mode,
Â  Â  Â  Â  Â  Â  Â  Â  corpus_name # <-- Unique ID for cache invalidation
Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  st.session_state['n_gram_results_df'] = n_gram_df.copy()
Â  Â  Â  Â  Â  Â Â 
Â  Â  n_gram_df = st.session_state['n_gram_results_df']
Â  Â Â 
Â  Â  if n_gram_df.empty:
Â  Â  Â  Â  st.warning("No N-grams found matching the criteria. Adjust the N-Gram size or clear filters in the sidebar.")
Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  st.success(f"Found **{len(n_gram_df):,}** unique {st.session_state['n_gram_size']}-grams matching the criteria.")
Â  Â Â 
Â  Â  # --- Display Table ---
Â  Â  st.markdown("---")
Â  Â  st.subheader(f"Top N-Grams (Showing {min(100, len(n_gram_df))})")
Â  Â Â 
Â  Â  n_gram_display_df = n_gram_df.head(100).copy()
Â  Â Â 
Â  Â  # Custom CSS for scrollable tables (Max 100 entries)
Â  Â  scroll_style = f"""
Â  Â  <style>
Â  Â  .scrollable-table {{
Â  Â  Â  Â  max-height: 400px; /* Fixed height for 100 entries max */
Â  Â  Â  Â  overflow-y: auto;
Â  Â  }}
Â  Â  </style>
Â  Â  """
Â  Â  st.markdown(scroll_style, unsafe_allow_html=True)
Â  Â Â 
Â  Â  # Use a scrollable container
Â  Â  html_table = n_gram_display_df.to_html(index=False, classes=['n-gram-table'])
Â  Â  # FIX 3.1: Change to triple quotes for robustness
Â  Â  st.markdown(f"""<div class='scrollable-table'>{html_table}</div>""", unsafe_allow_html=True)

Â  Â  # --- Download Button ---
Â  Â  st.markdown("---")
Â  Â  st.subheader("Download Full Results")
Â  Â Â 
Â  Â  download_label = (
Â  Â  Â  Â  f"â¬‡ Download Full {st.session_state['n_gram_size']}-Gram List "
Â  Â  Â  Â  f"({len(n_gram_df):,} entries) (xlsx)"
Â  Â  )
Â  Â Â 
Â  Â  st.download_button(
Â  Â  Â  Â  download_label,
Â  Â  Â  Â  data=df_to_excel_bytes(n_gram_df),Â 
Â  Â  Â  Â  file_name=f"{st.session_state['n_gram_size']}-gram_full_list.xlsx",Â 
Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
Â  Â  )

# -----------------------------------------------------
# MODULE: CONCORDANCE LOGIC
# -----------------------------------------------------
if st.session_state['view'] == 'concordance' and st.session_state.get('analyze_btn', False) and st.session_state.get('typed_target_input'):
Â  Â Â 
Â  Â  # Get current parameters
Â  Â  kwic_left = st.session_state.get('kwic_left', 7)
Â  Â  kwic_right = st.session_state.get('kwic_right', 7)
Â  Â  pattern_collocate = st.session_state.get('pattern_collocate_input', '').lower().strip()
Â  Â  pattern_collocate_pos = st.session_state.get('pattern_collocate_pos_input', '').strip()Â 
Â  Â  pattern_window = st.session_state.get('pattern_search_window', 0)
Â  Â Â 
Â  Â  is_pattern_search_active = pattern_collocate or pattern_collocate_pos
Â  Â  is_parallel_mode = st.session_state.get('parallel_mode', False)
Â  Â  target_sent_map = st.session_state.get('target_sent_map', {})
Â  Â Â 
Â  Â  # Display settings
Â  Â  show_pos_tag = st.session_state['show_pos_tag']
Â  Â  show_lemma = st.session_state['show_lemma']
Â  Â Â 
Â  Â  # Generate KWIC lines using the reusable function
Â  Â  with st.spinner("Searching corpus and generating concordance..."):
Â  Â  Â  Â  # KWIC returns (kwic_rows, total_matches, raw_target_input, literal_freq, list_of_sent_ids, breakdown_df)
Â  Â  Â  Â  kwic_rows, total_matches, raw_target_input, literal_freq, sent_ids, breakdown_df = generate_kwic(
Â  Â  Â  Â  Â  Â  df, st.session_state['typed_target_input'], kwic_left, kwic_right,Â 
Â  Â  Â  Â  Â  Â  pattern_collocate if is_pattern_search_active else "",Â 
Â  Â  Â  Â  Â  Â  pattern_collocate_pos if is_pattern_search_active else "",Â 
Â  Â  Â  Â  Â  Â  pattern_window if is_pattern_search_active else 0,
Â  Â  Â  Â  Â  Â  limit=KWIC_MAX_DISPLAY_LINES,
Â  Â  Â  Â  Â  Â  is_parallel_mode=is_parallel_mode, # Pass parallel flag
Â  Â  Â  Â  Â  Â  show_pos=show_pos_tag,Â 
Â  Â  Â  Â  Â  Â  show_lemma=show_lemma
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  if literal_freq == 0: # Check literal_freq here, not total_matches, for consistency with breakdown
Â  Â  Â  Â  st.warning(f"Target '{raw_target_input}' not found in corpus.")
Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  # Prepare metadata for display
Â  Â  rel_freq = (literal_freq / total_tokens) * 1_000_100
Â  Â Â 
Â  Â  # --- MODIFICATION: Renamed column and updated value ---
Â  Â  wildcard_freq_df = pd.DataFrame([{"Query Result": raw_target_input, "Absolute Frequency": literal_freq, "Relative Frequency (per M)": f"{rel_freq:.4f}"}])
Â  Â  results_df = wildcard_freq_dfÂ 

Â  Â  # --- KWIC Display ---
Â  Â  st.subheader("ðŸ“š Concordance Results")
Â  Â Â 
Â  Â  if is_pattern_search_active:
Â  Â  Â  Â  st.success(f"Pattern search successful! Found **{total_matches}** filtered instances of '{raw_target_input}' co-occurring with the specified criteria. POS/Lemma Display: **{show_pos_tag}**/**{show_lemma}**.")
Â  Â  else:
Â  Â  Â  Â  st.success(f"Found **{literal_freq}** total occurrences of the primary target word matching the criteria. POS/Lemma Display: **{show_pos_tag}**/**{show_lemma}**.")
Â  Â Â 
Â  Â  # --- LLM INTERPRETATION BUTTON/EXPANDER ---
Â  Â  if st.button("ðŸ§  Interpret Concordance Results (LLM)", key="llm_concordance_btn"):
Â  Â  Â  Â  kwic_df_for_llm = pd.DataFrame(kwic_rows).head(10).copy().drop(columns=['Collocate'])
Â  Â  Â  Â  interpret_results_llm(raw_target_input, "Concordance", "KWIC Context Sample (Max 10 lines)", kwic_df_for_llm)

Â  Â  if st.session_state['llm_interpretation_result']:
Â  Â  Â  Â  with st.expander("LLM Interpretation (Feature Disabled)", expanded=True):
Â  Â  Â  Â  Â  Â  st.markdown(st.session_state['llm_interpretation_result'])
Â  Â  Â  Â  st.markdown("---")
Â  Â  # ----------------------------------------
Â  Â Â 
Â  Â  # --- NEW POSITION: TARGET FREQUENCY (Full Width) ---
Â  Â  st.subheader(f"Target Query Summary")
Â  Â  st.dataframe(results_df, use_container_width=True, hide_index=True)
Â  Â Â 
Â  Â  st.markdown("---") # Separator

Â  Â  # --- NEW: Breakdown of Matching Forms (v17.50: Finalized User-Specified Dark Theme Styling) ---
Â  Â  if not breakdown_df.empty:
Â  Â  Â  Â  st.subheader(f"Token Breakdown for Query '{raw_target_input}'")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display max 100 entries
Â  Â  Â  Â  breakdown_display_df = breakdown_df.head(100).copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use a scrollable container and apply specific table styling
Â  Â  Â  Â  scroll_style_breakdown = f"""
Â  Â  Â  Â  <style>
Â  Â  Â  Â  /* Container for scroll */
Â  Â  Â  Â  .scrollable-breakdown-table {{
Â  Â  Â  Â  Â  Â  max-height: 300px;
Â  Â  Â  Â  Â  Â  overflow-y: auto;
Â  Â  Â  Â  Â  Â  border: 1px solid #444444; /* Use header background color for border */
Â  Â  Â  Â  }}
Â  Â  Â  Â  /* Style the table and cells */
Â  Â  Â  Â  .breakdown-table {{
Â  Â  Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  Â  Â  border-collapse: collapse;
Â  Â  Â  Â  Â  Â  font-size: 0.9em;
Â  Â  Â  Â  }}
Â  Â  Â  Â  .breakdown-table th {{
Â  Â  Â  Â  Â  Â  background-color: #444444; /* User's Header Background */
Â  Â  Â  Â  Â  Â  color: #FAFAFA;Â  Â  Â  Â  Â  Â /* User's Text Color */
Â  Â  Â  Â  Â  Â  padding: 8px;
Â  Â  Â  Â  Â  Â  text-align: left;
Â  Â  Â  Â  }}
Â  Â  Â  Â  .breakdown-table td {{
Â  Â  Â  Â  Â  Â  background-color: #1F1F1F; /* *** FIXED ROW BACKGROUND: Matches KWIC/App BG *** */
Â  Â  Â  Â  Â  Â  color: #FAFAFA;Â  Â  Â  Â  Â  Â /* User's Text Color */
Â  Â  Â  Â  Â  Â  padding: 8px;
Â  Â  Â  Â  Â  Â  border-bottom: 1px solid #333;
Â  Â  Â  Â  }}
Â  Â  Â  Â  /* *** CRITICAL FIX: Forces Text Color and Background on ALL 4 columns, including the new Zipf column (4th) *** */
Â  Â  Â  Â  .breakdown-table td:nth-child(1), .breakdown-table td:nth-child(2), .breakdown-table td:nth-child(3), .breakdown-table td:nth-child(4) {{
Â  Â  Â  Â  Â  Â  background-color: #1F1F1F !important; /* Forces dark background to hide Streamlit numeric column style */
Â  Â  Â  Â  Â  Â  color: #FAFAFA !important; /* Forces visible white text */
Â  Â  Â  Â  }}
Â  Â  Â  Â  </style>
Â  Â  Â  Â  """
Â  Â  Â  Â  st.markdown(scroll_style_breakdown, unsafe_allow_html=True)

Â  Â  Â  Â  # Apply the CSS class 'breakdown-table' to the generated HTML
Â  Â  Â  Â  html_table_breakdown = breakdown_display_df.to_html(index=False, classes=['breakdown-table'])
Â  Â  Â  Â  st.markdown(f"""<div class='scrollable-breakdown-table'>{html_table_breakdown}</div>""", unsafe_allow_html=True)
Â  Â  Â  Â  # --- END MODIFIED CSS ---
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  "â¬‡ Download full token breakdown (xlsx)",Â 
Â  Â  Â  Â  Â  Â  data=df_to_excel_bytes(breakdown_df),Â 
Â  Â  Â  Â  Â  Â  file_name=f"{raw_target_input.replace(' ', '_')}_token_breakdown.xlsx",Â 
Â  Â  Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
Â  Â  Â  Â  )
Â  Â  Â  Â  st.markdown("---")
Â  Â  # -----------------------------------------

Â  Â  # --- KWIC Display (Now Full Width) ---
Â  Â  # NOTE: The KWIC display logic uses total_matches, which accounts for the collocate pattern filter.
Â  Â  st.subheader(f"Concordance (KWIC) â€” top {len(kwic_rows)} lines (Scrollable max {KWIC_MAX_DISPLAY_LINES})")
Â  Â  Â  Â Â 
Â  Â  kwic_df = pd.DataFrame(kwic_rows).drop(columns=['Collocate'])
Â  Â  kwic_preview = kwic_df.copy().reset_index(drop=True)
Â  Â  kwic_preview.insert(0, "No", range(1, len(kwic_preview)+1))
Â  Â Â 
Â  Â  # --- NEW: Add Translation Column ---
Â  Â  if is_parallel_mode:
Â  Â  Â  Â  translations = [st.session_state['target_sent_map'].get(sent_id, "TRANSLATION N/A") for sent_id in sent_ids]
Â  Â  Â  Â  kwic_preview[f'Translation ({TARGET_LANG_CODE})'] = translations
Â  Â Â 
Â  Â  # --- KWIC Table Style (REVISED FOR EXPLICIT FLEXIBLE COLUMN WIDTHS) ---
Â  Â  kwic_table_style = f"""
Â  Â  Â  Â  Â  Â <style>
Â  Â  Â  Â  Â  Â .dataframe-container-scroll {{
Â  Â  Â  Â  Â  Â  Â  Â max-height: 400px; /* Fixed vertical height */
Â  Â  Â  Â  Â  Â  Â  Â overflow-y: auto;
Â  Â  Â  Â  Â  Â  Â  Â margin-bottom: 1rem;
Â  Â  Â  Â  Â  Â  Â  Â width: 100%;
Â  Â  Â  Â  Â  Â }}
Â  Â  Â  Â  Â  Â .dataframe table {{Â 
Â  Â  Â  Â  Â  Â  Â  Â width: 100%;Â 
Â  Â  Â  Â  Â  Â  Â  Â table-layout: fixed; /* Use fixed layout to enforce proportional width */
Â  Â  Â  Â  Â  Â  Â  Â font-family: monospace;Â 
Â  Â  Â  Â  Â  Â  Â  Â color: white;
Â  Â  Â  Â  Â  Â  Â  Â font-size: 0.9em;
Â  Â  Â  Â  Â  Â }}
Â  Â  Â  Â  Â  Â .dataframe th {{ font-weight: bold; text-align: center; white-space: nowrap; }}
Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â /* KWIC Width Fix: Set proportional column widths (ensures full width is used even without POS/Lemma) */
Â  Â  Â  Â  Â  Â .dataframe td:nth-child(1) {{ width: 5%; }} /* No column */
Â  Â  Â  Â  Â  Â .dataframe td:nth-child(2) {{ width: 40%; text-align: right; }} /* Left context */
Â  Â  Â  Â  Â  Â .dataframe td:nth-child(3) {{Â 
Â  Â  Â  Â  Â  Â  Â  Â width: 15%; /* Node */
Â  Â  Â  Â  Â  Â  Â  Â text-align: center;Â 
Â  Â  Â  Â  Â  Â  Â  Â font-weight: bold;Â 
Â  Â  Â  Â  Â  Â  Â  Â background-color: #f0f0f0;Â 
Â  Â  Â  Â  Â  Â  Â  Â color: black;Â 
Â  Â  Â  Â  Â  Â }}Â 
Â  Â  Â  Â  Â  Â .dataframe td:nth-child(4) {{ width: 40%; text-align: left; }} /* Right context */
Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â /* Ensure content can wrap */
Â  Â  Â  Â  Â  Â .dataframe td:nth-child(2), .dataframe td:nth-child(3), .dataframe td:nth-child(4) {{Â 
Â  Â  Â  Â  Â  Â  Â  Â white-space: normal;
Â  Â  Â  Â  Â  Â  Â  Â vertical-align: top;
Â  Â  Â  Â  Â  Â  Â  Â padding: 5px 10px;
Â  Â  Â  Â  Â  Â  Â  Â line-height: 1.5;Â 
Â  Â  Â  Â  Â  Â }}
Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â /* Adjust for Translation column if present (total is 100%) */
Â  Â  Â  Â  Â  Â .dataframe th:nth-last-child(1) {{ width: 10%; }} /* Translation Column */
Â  Â  Â  Â  Â  Â .dataframe td:nth-last-child(1) {{ text-align: left; color: #CCFFCC; font-family: sans-serif; font-size: 0.8em; white-space: normal; }}

Â  Â  Â  Â  Â  Â </style>
Â  Â  """
Â  Â  st.markdown(kwic_table_style, unsafe_allow_html=True)
Â  Â Â 
Â  Â  # Use HTML table and escape=False to preserve the HTML formatting (inline styles)
Â  Â  html_table = kwic_preview.to_html(escape=False, classes=['dataframe'], index=False)
Â  Â  scrollable_html = f"""<div class='dataframe-container-scroll'>{html_table}</div>"""

Â  Â  st.markdown(scrollable_html, unsafe_allow_html=True)

Â  Â  st.caption("Note: Pattern search collocates are **bolded and highlighted bright yellow**.")
Â  Â  st.download_button("â¬‡ Download full concordance (xlsx)", data=df_to_excel_bytes(kwic_preview), file_name=f"{raw_target_input.replace(' ', '_')}_full_concordance.xlsx", mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")


# -----------------------------------------------------
# MODULE: DICTIONARY
# -----------------------------------------------------
if st.session_state['view'] == 'dictionary':
Â  Â Â 
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"ðŸ“˜ Dictionary Lookup{lang_display_suffix}")
Â  Â Â 
Â  Â  # --- Input and Analysis Trigger (Automatic on Change) ---
Â  Â  # FIX: Use conditional language suffix
Â  Â  current_dict_word = st.text_input(
Â  Â  Â  Â  f"Enter a Token/Word to lookup (e.g., 'sessions'){lang_input_suffix}:",Â 
Â  Â  Â  Â  value=st.session_state.get('dict_word_input_main', ''),
Â  Â  Â  Â  key="dict_word_input_main",
Â  Â  ).strip()
Â  Â Â 
Â  Â  if not current_dict_word:
Â  Â  Â  Â  st.info("Enter a word to view its linguistic summary, examples, and collocates. Analysis runs automatically.")
Â  Â  Â  Â  st.button("ðŸ”Ž Manual Re-Analyze", key="manual_dict_analyze_disabled", disabled=True)
Â  Â  Â  Â  st.stop()
Â  Â Â 
Â  Â  st.button("ðŸ”Ž Manual Re-Analyze", key="manual_dict_analyze")
Â  Â  Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- 1. Consolidated Word Forms by Lemma ---
Â  Â  if is_raw_mode or 'lemma' not in df.columns:
Â  Â  Â  Â  st.warning("Lemma and POS analysis is disabled because the corpus is not tagged/lemmatized. The corpus needs to be uploaded as a T/P/L vertical file.")
Â  Â  Â  Â  forms_list = pd.DataFrame()
Â  Â  Â  Â  unique_lemma_list = []
Â  Â  else:
Â  Â  Â  Â  forms_list, unique_pos_list, unique_lemma_list = get_all_lemma_forms_details(df, current_dict_word)

Â  Â  # MODIFIED: Changed header to "Word Forms" globally
Â  Â  st.subheader(f"Word Forms")

Â  Â  # --------------------------------------------------------
Â  Â  # IPA/CEFR/Pronunciation Feature Logic (REVISED for KBBI/Cambridge)
Â  Â  # --------------------------------------------------------
Â  Â  corpus_lang = SOURCE_LANG_CODE.upper()Â 
Â  Â Â 
Â  Â  english_langs = ('EN', 'ENG', 'ENGLISH')Â 
Â  Â  indonesian_langs = ('ID', 'INDONESIAN')
Â  Â Â 
Â  Â  is_english_corpus = corpus_lang in english_langs
Â  Â  is_indonesian_corpus = corpus_lang in indonesian_langs
Â  Â Â 
Â  Â  # IPA/CEFR are enabled ONLY for English.
Â  Â  ipa_active = IPA_FEATURE_AVAILABLE and is_english_corpus
Â  Â  cefr_active = CEFR_FEATURE_AVAILABLE and is_english_corpus
Â  Â Â 
Â  Â  # Calculate initial frequency to determine if the token exists at all
Â  Â  word_freq = token_counts.get(current_dict_word.lower(), 0)

Â  Â  # --- GLOBAL FIX: Force Forms List Generation in Raw Mode ---
Â  Â  # This section ensures the table appears if the token is found, regardless of POS/Lemma tags.
Â  Â  if forms_list.empty and word_freq > 0:
Â  Â  Â  Â  # Manufacturing the form list for display (as requested)
Â  Â  Â  Â  forms_list = pd.DataFrame([{
Â  Â  Â  Â  Â  Â  'token': current_dict_word,
Â  Â  Â  Â  Â  Â  'pos': '##',
Â  Â  Â  Â  Â  Â  'lemma': '##'
Â  Â  Â  Â  }])
Â  Â  # ------------------------------------------------------------------

Â  Â  # If the word wasn't found at all (freq is 0 and the manufactured list is still empty)
Â  Â  if forms_list.empty:
Â  Â  Â  Â  st.warning(f"Token **'{current_dict_word}'** not found in the corpus.")
Â  Â  Â  Â  st.stop()

Â  Â  # --- Table Generation (Runs for Tagged or Forced Raw Mode) ---
Â  Â  forms_list.rename(columns={
Â  Â  Â  Â  'token': 'Token',Â 
Â  Â  Â  Â  'pos': 'POS Tag',Â 
Â  Â  Â  Â  'lemma': 'Lemma'
Â  Â  }, inplace=True)
Â  Â  Â  Â Â 
Â  Â  # --- ADD FREQUENCY COLUMNS (Absolute and Relative) ---
Â  Â  forms_list.insert(forms_list.shape[1], 'Absolute Frequency', forms_list['Token'].apply(lambda t: token_counts.get(t.lower(), 0)))
Â  Â  forms_list.insert(forms_list.shape[1], 'Relative Frequency (per M)', forms_list['Absolute Frequency'].apply(lambda f: round((f / total_tokens) * 1_000_000, 4)))
Â  Â Â 
Â  Â Â 
Â  Â  # ------------------ ZIPF BAND CALCULATION (NEW) ------------------
Â  Â  # Calculate Zipf score from Relative Frequency (which is PMW)
Â  Â  forms_list.insert(forms_list.shape[1], 'Zipf Score', forms_list['Relative Frequency (per M)'].apply(pmw_to_zipf).round(2))
Â  Â Â 
Â  Â  # Assign Zipf Band (1-5)
Â  Â  forms_list.insert(forms_list.shape[1], 'Zipf Band (1-5)', forms_list['Zipf Score'].apply(zipf_to_band))
Â  Â  # -----------------------------------------------------------
Â  Â Â 
Â  Â  # ------------------ CEFR Column Insertion (FIXED) ------------------
Â  Â  if cefr_active: # This runs only if is_english_corpus is True
Â  Â  Â  Â Â 
Â  Â  Â  Â  def safe_get_cefr(token):
Â  Â  Â  Â  Â  Â  """Safely calls CEFR_ANALYZER and catches exceptions from cefrpy library."""
Â  Â  Â  Â  Â  Â  # Use 'NA' as the standard placeholder for uncategorized words
Â  Â  Â  Â  Â  Â  if not CEFR_ANALYZER:
Â  Â  Â  Â  Â  Â  Â  Â  return 'NA'
Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  # The token is already lowercased by the main frequency counter, but applying lower() again for robustness
Â  Â  Â  Â  Â  Â  Â  Â  level = CEFR_ANALYZER.get_cefr_level(token).upper()
Â  Â  Â  Â  Â  Â  Â  Â  return level if level != 'NA' else 'NA' # Ensure NA is used for uncategorized
Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  # Catch any error (e.g., word not found, internal library error)
Â  Â  Â  Â  Â  Â  Â  Â  return 'NA'

Â  Â  Â  Â  forms_list.insert(forms_list.shape[1], 'CEFR', forms_list['Token'].apply(safe_get_cefr))
Â  Â  Â  Â Â 
Â  Â  else: # Ensure column is present with 'NA' placeholder
Â  Â  Â  Â  forms_list.insert(forms_list.shape[1], 'CEFR', 'NA')
Â  Â  # -----------------------------------------------------------
Â  Â Â 
Â  Â  # ------------------ IPA Column Insertion ------------------
Â  Â  if ipa_active: # This runs only if is_english_corpus is True
Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  def get_ipa_transcription(token):
Â  Â  Â  Â  Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  import eng_to_ipa as ipaÂ 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return ipa.convert(token)
Â  Â  Â  Â  Â  Â  Â  Â  except Exception:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  return "NA"Â 
Â  Â  Â  Â  Â  Â  forms_list.insert(forms_list.shape[1], 'IPA Transcription', forms_list['Token'].apply(get_ipa_transcription))
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Error during IPA transcription: {e}")
Â  Â  Â  Â  Â  Â  ipa_active = FalseÂ 
Â  Â  Â  Â  Â  Â Â 
Â  Â  # Ensure column is present with 'NA' placeholder if not active/failed
Â  Â  if not ipa_active:
Â  Â  Â  Â  forms_list.insert(forms_list.shape[1], 'IPA Transcription', 'NA')
Â  Â  # -----------------------------------------------------------
Â  Â  Â  Â  Â  Â Â 
Â  Â  # --- Pronunciation Link Logic (REVISED for KBBI/Cambridge) ---
Â  Â  if is_indonesian_corpus: # This runs if corpus_lang is ID
Â  Â  Â  Â  # Use KBBI for Indonesian dictionary
Â  Â  Â  Â  pronunciation_url = lambda token: f"https://kbbi.kemdikbud.go.id/entri/{token.lower()}"
Â  Â  Â  Â  pronunciation_label = f"Dictionary ({corpus_lang} - KBBI)"
Â  Â  elif is_english_corpus:
Â  Â  Â  Â  # Use Cambridge Dictionary for English
Â  Â  Â  Â  pronunciation_url = lambda token: f"https://dictionary.cambridge.org/dictionary/english/{token.lower()}"
Â  Â  Â  Â  pronunciation_label = "Dictionary (EN - Cambridge)"
Â  Â  else:
Â  Â  Â  Â  # Generic link for other languages / Fallback
Â  Â  Â  Â  pronunciation_url = lambda token: f"https://forvo.com/word/{token}/#{corpus_lang.lower()}"
Â  Â  Â  Â  pronunciation_label = f"Pronunciation/Dictionary ({corpus_lang})"


Â  Â  # Create a new column with the clickable link HTML
Â  Â  # The column name is the 'pronunciation_label' determined above (e.g., 'Dictionary (ID - KBBI)')
Â  Â  forms_list.insert(forms_list.shape[1], pronunciation_label, forms_list['Token'].apply(
Â  Â  Â  Â  lambda token: f"<a href='{pronunciation_url(token)}' target='_blank'>Link</a>" # Changed to 'Link' for brevity
Â  Â  ))
Â  Â Â 
Â  Â  # Define table styling for cleaner look with markdown
Â  Â  html_style = """
Â  Â  <style>
Â  Â  .forms-list-table {
Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  border-collapse: collapse;
Â  Â  Â  Â  font-family: Arial, sans-serif;
Â  Â  Â  Â  font-size: 0.9em;
Â  Â  }
Â  Â  .forms-list-table th {
Â  Â  Â  Â  background-color: #383838;
Â  Â  Â  Â  color: white;
Â  Â  Â  Â  padding: 8px;
Â  Â  Â  Â  text-align: left;
Â  Â  }
Â  Â  .forms-list-table td {
Â  Â  Â  Â  padding: 8px;
Â  Â  Â  Â  border-bottom: 1px solid #444444;
Â  Â  }
Â  Â  .forms-list-table tr:hover {
Â  Â  Â  Â  background-color: #333333;
Â  Â  }
Â  Â  </style>
Â  Â  """
Â  Â  st.markdown(html_style, unsafe_allow_html=True)

Â  Â  st.markdown(
Â  Â  Â  Â  forms_list.to_html(index=False, escape=False, classes=['forms-list-table']),Â 
Â  Â  Â  Â  unsafe_allow_html=True
Â  Â  )
Â  Â Â 
Â  Â  # Feature info messages based on current language
Â  Â  if not IPA_FEATURE_AVAILABLE and is_english_corpus:
Â  Â  Â  Â  st.info("ðŸ’¡ **Phonetic Transcription (IPA) feature requires the `eng-to-ipa` library to be installed** (`pip install eng-to-ipa`).")
Â  Â  elif not is_english_corpus and IPA_FEATURE_AVAILABLE:
Â  Â  Â  Â  st.info(f"ðŸ’¡ Phonetic Transcription (IPA) is disabled for non-English corpus ({SOURCE_LANG_CODE}).")
Â  Â  Â  Â Â 
Â  Â  if not CEFR_FEATURE_AVAILABLE and is_english_corpus:
Â  Â  Â  Â  st.info("ðŸ’¡ **CEFR Categorization feature requires the `cefrpy` library to be installed** (`pip install cefrpy`).")
Â  Â  Â  Â Â 
Â  Â  st.markdown("---")

Â  Â  # --- 2. Related Forms (by Regex) ---
Â  Â  st.subheader("Related Forms (by Regex)")
Â  Â Â 
Â  Â  related_regex_forms = get_related_forms_by_regex(df, current_dict_word)
Â  Â Â 
Â  Â  if related_regex_forms:
Â  Â  Â  Â  st.markdown(f"**Tokens matching the pattern `*.{current_dict_word}.*` (case insensitive):**")
Â  Â  Â  Â  st.text_area(
Â  Â  Â  Â  Â  Â  "Related Forms (by regex)",Â 
Â  Â  Â  Â  Â  Â  ", ".join(related_regex_forms),Â 
Â  Â  Â  Â  Â  Â  height=100,Â 
Â  Â  Â  Â  Â  Â  key=f"regex_forms_output_{current_dict_word}"Â 
Â  Â  Â  Â  )
Â  Â  else:
Â  Â  Â  Â  st.info(f"No related tokens found matching the regex pattern.")
Â  Â  Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- 3. Random Concordance Examples ---
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"Random Examples (Concordance{lang_display_suffix})")
Â  Â Â 
Â  Â  kwic_left = st.session_state.get('kwic_left', 7)
Â  Â  kwic_right = st.session_state.get('kwic_right', 7)
Â  Â  is_parallel_mode = st.session_state.get('parallel_mode', False)
Â  Â  target_sent_map = st.session_state.get('target_sent_map', {})
Â  Â Â 
Â  Â  # Display settings
Â  Â  show_pos_tag = st.session_state['show_pos_tag']
Â  Â  show_lemma = st.session_state['show_lemma']

Â  Â  with st.spinner(f"Fetching random concordance examples for '{current_dict_word}'..."):
Â  Â  Â  Â  # KWIC returns (kwic_rows, total_matches, raw_target_input, literal_freq, sent_ids, breakdown_df)
Â  Â  Â  Â  kwic_rows, total_matches, _, _, sent_ids, _ = generate_kwic(
Â  Â  Â  Â  Â  Â  df, current_dict_word, kwic_left, kwic_right,Â 
Â  Â  Â  Â  Â  Â  random_sample=True, limit=KWIC_MAX_DISPLAY_LINES,
Â  Â  Â  Â  Â  Â  is_parallel_mode=is_parallel_mode,
Â  Â  Â  Â  Â  Â  show_pos=show_pos_tag,
Â  Â  Â  Â  Â  Â  show_lemma=show_lemma
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  if kwic_rows:
Â  Â  Â  Â  display_limit = min(5, len(kwic_rows))
Â  Â  Â  Â  st.success(f"Showing {display_limit} random examples from {total_matches:,} total matches. POS/Lemma Display: **{show_pos_tag}**/**{show_lemma}**.")
Â  Â  Â  Â Â 
Â  Â  Â  Â  kwic_df = pd.DataFrame(kwic_rows).drop(columns=['Collocate'])
Â  Â  Â  Â  kwic_preview = kwic_df.copy().reset_index(drop=True)
Â  Â  Â  Â  kwic_preview.insert(0, "No", range(1, len(kwic_preview)+1))
Â  Â  Â  Â Â 
Â  Â  Â  Â  # --- NEW: Add Translation Column ---
Â  Â  Â  Â  if is_parallel_mode:
Â  Â  Â  Â  Â  Â  translations = [st.session_state['target_sent_map'].get(sent_id, "TRANSLATION N/A") for sent_id in sent_ids]
Â  Â  Â  Â  Â  Â  kwic_preview[f'Translation ({TARGET_LANG_CODE})'] = translations

Â  Â  Â  Â  kwic_table_style = f"""
Â  Â  Â  Â  Â  Â  <style>
Â  Â  Â  Â  Â  Â  .dictionary-kwic-container {{
Â  Â  Â  Â  Â  Â  Â  Â  max-height: 250px; /* Fixed vertical height */
Â  Â  Â  Â  Â  Â  Â  Â  overflow-y: auto;
Â  Â  Â  Â  Â  Â  Â  Â  margin-bottom: 1rem;
Â  Â  Â  Â  Â  Â  Â  Â  width: 100%;
Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  Â  Â  .dict-kwic-table table {{Â 
Â  Â  Â  Â  Â  Â  Â  Â  width: 100%;Â 
Â  Â  Â  Â  Â  Â  Â  Â  table-layout: fixed; /* Fixed layout to enforce proportional width */
Â  Â  Â  Â  Â  Â  Â  Â  font-family: monospace;Â 
Â  Â  Â  Â  Â  Â  Â  Â  color: white;
Â  Â  Â  Â  Â  Â  Â  Â  font-size: 0.9em;
Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  Â  Â  .dict-kwic-table th {{ font-weight: bold; text-align: center; white-space: nowrap; }}
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  /* KWIC Width Fix: Set proportional column widths */
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-child(1) {{ width: 5%; }} /* No column */
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-child(2) {{ width: 40%; text-align: right; }} /* Left context */
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-child(3) {{Â 
Â  Â  Â  Â  Â  Â  Â  Â  width: 15%; /* Node */
Â  Â  Â  Â  Â  Â  Â  Â  text-align: center;Â 
Â  Â  Â  Â  Â  Â  Â  Â  font-weight: bold;Â 
Â  Â  Â  Â  Â  Â  Â  Â  background-color: #f0f0f0;Â 
Â  Â  Â  Â  Â  Â  Â  Â  color: black;Â 
Â  Â  Â  Â  Â  Â  }}Â 
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-child(4) {{ width: 40%; text-align: left; }} /* Right context */
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  /* Ensure content can wrap */
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-child(2), .dict-kwic-table td:nth-child(3), .dict-kwic-table td:nth-child(4) {{Â 
Â  Â  Â  Â  Â  Â  Â  Â  white-space: normal;
Â  Â  Â  Â  Â  Â  Â  Â  vertical-align: top;
Â  Â  Â  Â  Â  Â  Â  Â  padding: 5px 10px;
Â  Â  Â  Â  Â  Â  Â  Â  line-height: 1.5;
Â  Â  Â  Â  Â  Â  }}
Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  /* Adjust for Translation column if present (total is 100%) */
Â  Â  Â  Â  Â  Â  .dict-kwic-table th:nth-last-child(1) {{ width: 10%; }} /* Translation Column */
Â  Â  Â  Â  Â  Â  .dict-kwic-table td:nth-last-child(1) {{ text-align: left; color: #CCFFCC; font-family: sans-serif; font-size: 0.8em; white-space: normal; }}

Â  Â  Â  Â  Â  Â  </style>
Â  Â  Â  Â  """
Â  Â  Â  Â  st.markdown(kwic_table_style, unsafe_allow_html=True)
Â  Â  Â  Â Â 
Â  Â  Â  Â  html_table = kwic_preview.to_html(escape=False, classes=['dict-kwic-table'], index=False)
Â  Â  Â  Â  # FIX 2: Change to triple quotes for robustness
Â  Â  Â  Â  scrollable_html = f"""<div class='dictionary-kwic-container'>{html_table}</div>"""
Â  Â  Â  Â  st.markdown(scrollable_html, unsafe_allow_html=True)
Â  Â  else:
Â  Â  Â  Â  st.info("No examples found.")
Â  Â  Â  Â Â 
Â  Â  st.markdown("---")

Â  Â  # --- 4. Collocates and Collocate Examples ---
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"Collocation Analysis{lang_display_suffix}")
Â  Â Â 
Â  Â  coll_window = st.session_state.get('coll_window', 5)
Â  Â  mi_min_freq = st.session_state.get('mi_min_freq', 1)
Â  Â  max_collocates = st.session_state.get('max_collocates', 20)
Â  Â Â 
Â  Â  collocate_regex = st.session_state.get('collocate_regex_input', '').lower().strip()
Â  Â  collocate_pos_regex_input = st.session_state.get('collocate_pos_regex_input_coll', '').strip()
Â  Â  selected_pos_tags = st.session_state.get('selected_pos_tags_input', [])
Â  Â  collocate_lemma = st.session_state.get('collocate_lemma_input', '').lower().strip()
Â  Â Â 
Â  Â  with st.spinner(f"Running collocation analysis (window Â±{coll_window})..."):
Â  Â  Â  Â  stats_df_sorted, freq, primary_target_mwu = generate_collocation_results(
Â  Â  Â  Â  Â  Â  df, current_dict_word, coll_window, mi_min_freq, max_collocates, is_raw_mode,
Â  Â  Â  Â  Â  Â  collocate_regex, collocate_pos_regex_input, selected_pos_tags, collocate_lemma
Â  Â  Â  Â  )
Â  Â Â 
Â  Â  if stats_df_sorted.empty:
Â  Â  Â  Â  st.warning("No collocates found matching the criteria.")
Â  Â  Â  Â  # Only stop if the primary target itself wasn't found (handled above). Continue to show the rest of the dictionary info.
Â  Â  else:
Â  Â  Â  Â  top_collocates = stats_df_sorted.head(20)
Â  Â  Â  Â Â 
Â  Â  Â  Â  # 3a. Top Collocates List
Â  Â  Â  Â  collocate_list = ", ".join(top_collocates['Collocate'].tolist())
Â  Â  Â  Â  st.markdown(f"**Top {len(top_collocates)} Collocates (LL-ranked):**")
Â  Â  Â  Â  st.text_area("Collocate List", collocate_list, height=100)
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.markdown("---")
Â  Â  Â  Â  st.subheader(f"Collocate Examples (Top {len(top_collocates)} LL Collocates)")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use the dedicated KWIC display function (which now handles parallel mode)
Â  Â  Â  Â  display_collocation_kwic_examples(
Â  Â  Â  Â  Â  Â  df_corpus=df,Â 
Â  Â  Â  Â  Â  Â  node_word=current_dict_word,Â 
Â  Â  Â  Â  Â  Â  top_collocates_df=top_collocates,Â 
Â  Â  Â  Â  Â  Â  window=coll_window,
Â  Â  Â  Â  Â  Â  limit_per_collocate=1,
Â  Â  Â  Â  Â  Â  is_parallel_mode=is_parallel_mode,
Â  Â  Â  Â  Â  Â  target_sent_map=st.session_state['target_sent_map'],
Â  Â  Â  Â  Â  Â  show_pos=show_pos_tag,
Â  Â  Â  Â  Â  Â  show_lemma=show_lemma
Â  Â  Â  Â  )


# -----------------------------------------------------
# MODULE: COLLOCATION LOGIC
# -----------------------------------------------------
if st.session_state['view'] == 'collocation' and st.session_state.get('analyze_btn', False) and st.session_state.get('typed_target_input'):
Â  Â Â 
Â  Â  # Get Collocation Settings
Â  Â  coll_window = st.session_state.get('coll_window', 5)
Â  Â  mi_min_freq = st.session_state.get('mi_min_freq', 1)
Â  Â  max_collocates = st.session_state.get('max_collocates', 20)Â 
Â  Â Â 
Â  Â  # Get Filter Settings
Â  Â  collocate_regex = st.session_state.get('collocate_regex_input', '').lower().strip()
Â  Â  collocate_pos_regex_input = st.session_state.get('collocate_pos_regex_input_coll', '').strip()
Â  Â  selected_pos_tags = st.session_state.get('selected_pos_tags_input', [])
Â  Â  collocate_lemma = st.session_state.get('collocate_lemma_input', '').lower().strip()
Â  Â Â 
Â  Â  raw_target_input = st.session_state.get('typed_target_input')
Â  Â Â 
Â  Â  # Display settings
Â  Â  show_pos_tag = st.session_state['show_pos_tag']
Â  Â  show_lemma = st.session_state['show_lemma']
Â  Â Â 
Â  Â  with st.spinner("Running collocation analysis..."):
Â  Â  Â  Â  stats_df_sorted, freq, primary_target_mwu = generate_collocation_results(
Â  Â  Â  Â  Â  Â  df, raw_target_input, coll_window, mi_min_freq, max_collocates, is_raw_mode,
Â  Â  Â  Â  Â  Â  collocate_regex, collocate_pos_regex_input, selected_pos_tags, collocate_lemma
Â  Â  Â  Â  )

Â  Â  if freq == 0:
Â  Â  Â  Â  st.warning(f"Target '{raw_target_input}' not found in corpus.")
Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  primary_rel_freq = (freq / total_tokens) * 1_000_100
Â  Â Â 
Â  Â  # FIX: Use conditional language suffix
Â  Â  st.subheader(f"ðŸ”— Collocation Analysis Results{lang_display_suffix}")
Â  Â  st.success(f"Analyzing target '{primary_target_mwu}'. Frequency: **{freq:,}**, Relative Frequency: **{primary_rel_freq:.4f}** per million. POS/Lemma Display: **{show_pos_tag}**/**{show_lemma}**.")

Â  Â  if stats_df_sorted.empty:
Â  Â  Â  Â  st.warning("No collocates found after applying filters.")
Â  Â  Â  Â  st.stop()
Â  Â  Â  Â Â 
Â  Â  # --- LLM INTERPRETATION BUTTON/EXPANDER ---
Â  Â  if st.button("ðŸ§  Interpret Collocation Results (LLM)", key="llm_collocation_btn"):
Â  Â  Â  Â  # This LLM feature is still disabled/placeholder
Â  Â  Â  Â  interpret_results_llm(
Â  Â  Â  Â  Â  Â  target_word=raw_target_input,
Â  Â  Â  Â  Â  Â  analysis_type="Collocation",
Â  Â  Â  Â  Â  Â  data_description="Top Log-Likelihood Collocates",
Â  Â  Â  Â  Â  Â  data=stats_df_sorted[['Collocate', 'POS', 'Observed', 'LL', 'Direction']].head(10)
Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â Â 
Â  Â  if st.session_state['llm_interpretation_result']:
Â  Â  Â  Â  with st.expander("LLM Interpretation (Feature Disabled)", expanded=True):
Â  Â  Â  Â  Â  Â  st.markdown(st.session_state['llm_interpretation_result'])
Â  Â  Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- Graph Data ---
Â  Â  top_collocates_for_graphs = stats_df_sorted.head(max_collocates)
Â  Â  left_directional_df = top_collocates_for_graphs[top_collocates_for_graphs['Direction'].isin(['L', 'B'])].copy()
Â  Â  right_directional_df = top_collocates_for_graphs[top_collocates_for_graphs['Direction'].isin(['R', 'B'])].copy()

Â  Â  # --- DISPLAY GRAPHS SIDE BY SIDE ---
Â  Â  st.markdown("---")
Â  Â  st.subheader("Interactive Collocation Networks (Directional)")
Â  Â Â 
Â  Â  col_left_graph, col_right_graph = st.columns(2)
Â  Â Â 
Â  Â  # Only try to display if pyvis is available
Â  Â  if PYVIS_FEATURE_AVAILABLE:
Â  Â  Â  Â  with col_left_graph:
Â  Â  Â  Â  Â  Â  st.subheader(f"Left Collocates Only (Top {len(left_directional_df)} LL)")
Â  Â  Â  Â  Â  Â  if not left_directional_df.empty:
Â  Â  Â  Â  Â  Â  Â  Â  network_html_left = create_pyvis_graph(primary_target_mwu, left_directional_df)
Â  Â  Â  Â  Â  Â  Â  Â  components.html(network_html_left, height=450)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("No Left-dominant collocates found.")

Â  Â  Â  Â  with col_right_graph:
Â  Â  Â  Â  Â  Â  st.subheader(f"Right Collocates Only (Top {len(right_directional_df)} LL)")
Â  Â  Â  Â  Â  Â  if not right_directional_df.empty:
Â  Â  Â  Â  Â  Â  Â  Â  network_html_right = create_pyvis_graph(primary_target_mwu, right_directional_df)
Â  Â  Â  Â  Â  Â  Â  Â  components.html(network_html_right, height=450)
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  st.info("No Right-dominant collocates found.")
Â  Â  else:
Â  Â  Â  Â  st.warning("âš ï¸ **Network Graph Disabled:** The `pyvis` library is not available.")
Â  Â  Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  st.markdown(
Â  Â  Â  Â  """
Â  Â  Â  Â  **General Graph Key:** | Central Node (Target): **Yellow** | Collocate Node Color: Noun (N) **Green**, Verb (V) **Blue**, Adjective (J) **Pink**, Adverb (R) **Yellow**. | Bubble Size: Scales with Log-Likelihood (LL).
Â  Â  Â  Â  """
Â  Â  )
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # --- Full Tables (Max 100 entries, scrollable) ---
Â  Â  st.subheader(f"Collocation Tables â€” Top {min(100, len(stats_df_sorted))} LL/MI")
Â  Â Â 
Â  Â  # Filter to max 100 entries for display
Â  Â  full_ll = stats_df_sorted.head(100).copy().reset_index(drop=True)
Â  Â  full_ll.insert(0, "Rank", range(1, len(full_ll)+1))
Â  Â Â 
Â  Â  full_mi_all = stats_df_sorted[stats_df_sorted["Observed"] >= mi_min_freq].sort_values("MI", ascending=False).reset_index(drop=True)
Â  Â  full_mi = full_mi_all.head(100).copy()
Â  Â  full_mi.insert(0, "Rank", range(1, len(full_mi)+1))
Â  Â Â 
Â  Â  col_ll_table, col_mi_table = st.columns(2, gap="large")
Â  Â Â 
Â  Â  # --- Custom CSS for scrollable tables (Max 100 entries) ---
Â  Â  scroll_style = f"""
Â  Â  <style>
Â  Â  .scrollable-table {{
Â  Â  Â  Â  max-height: 400px; /* Fixed height for 100 entries max */
Â  Â  Â  Â  overflow-y: auto;
Â  Â  }}
Â  Â  </style>
Â  Â  """
Â  Â  st.markdown(scroll_style, unsafe_allow_html=True)
Â  Â Â 
Â  Â  with col_ll_table:
Â  Â  Â  Â  st.markdown(f"**Log-Likelihood (LL) (Top {len(full_ll)})**")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display table with relevant columns
Â  Â  Â  Â  ll_display_df = full_ll[['Rank', 'Collocate', 'LL', 'Direction', 'Significance']].copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use a scrollable container for the main table
Â  Â  Â  Â  html_table = ll_display_df.to_html(index=False, classes=['collocate-table'])
Â  Â  Â  Â  st.markdown(f"""<div class='scrollable-table'>{html_table}</div>""", unsafe_allow_html=True)
Â  Â  Â  Â Â 
Â  Â  with col_mi_table:
Â  Â  Â  Â  st.markdown(f"**Mutual Information (MI) (obs â‰¥ {mi_min_freq}, Top {len(full_mi)})**")
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Display table with relevant columns
Â  Â  Â  Â  mi_display_df = full_mi[['Rank', 'Collocate', 'MI', 'Direction', 'Significance']].copy()
Â  Â  Â  Â Â 
Â  Â  Â  Â  # Use a scrollable container for the main table
Â  Â  Â  Â  html_table = mi_display_df.to_html(index=False, classes=['collocate-table'])
Â  Â  Â  Â  st.markdown(f"""<div class='scrollable-table'>{html_table}</div>""", unsafe_allow_html=True)

Â  Â  # ---------- Download Buttons ----------
Â  Â  st.markdown("---")
Â  Â  st.subheader("Download Full Results")
Â  Â Â 
Â  Â  st.download_button(
Â  Â  Â  Â  f"â¬‡ Download full LL results (xlsx)",Â 
Â  Â  Â  Â  data=df_to_excel_bytes(stats_df_sorted),Â 
Â  Â  Â  Â  file_name=f"{primary_target_mwu.replace(' ', '_')}_LL_full_filtered.xlsx",Â 
Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
Â  Â  )
Â  Â  st.download_button(
Â  Â  Â  Â  f"â¬‡ Download full MI results (obsâ‰¥{mi_min_freq}) (xlsx)",Â 
Â  Â  Â  Â  data=df_to_excel_bytes(full_mi_all),Â 
Â  Â  Â  Â  file_name=f"{primary_target_mwu.replace(' ', '_')}_MI_full_obsge{mi_min_freq}_filtered.xlsx",Â 
Â  Â  Â  Â  mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet"
Â  Â  )
Â  Â Â 
Â  Â  # -----------------------------------------------------
Â  Â  # DEDICATED KWIC DISPLAY FOR TOP LL AND MI COLLOCATES
Â  Â  # -----------------------------------------------------
Â  Â  is_parallel_mode = st.session_state.get('parallel_mode', False)
Â  Â  target_sent_map = st.session_state.get('target_sent_map', {})

Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # LL-Ranked KWIC Examples
Â  Â  st.subheader(f"ðŸ“š Concordance Examples for Top {KWIC_COLLOC_DISPLAY_LIMIT} LL Collocates (1 example per collocate)")
Â  Â  display_collocation_kwic_examples(
Â  Â  Â  Â  df_corpus=df,Â 
Â  Â  Â  Â  node_word=primary_target_mwu,Â 
Â  Â  Â  Â  top_collocates_df=full_ll,Â 
Â  Â  Â  Â  window=coll_window,
Â  Â  Â  Â  limit_per_collocate=1,
Â  Â  Â  Â  is_parallel_mode=is_parallel_mode,
Â  Â  Â  Â  target_sent_map=st.session_state['target_sent_map'],
Â  Â  Â  Â  show_pos=show_pos_tag,
Â  Â  Â  Â  show_lemma=show_lemma
Â  Â  )
Â  Â Â 
Â  Â  st.markdown("---")
Â  Â Â 
Â  Â  # MI-Ranked KWIC Examples
Â  Â  st.subheader(f"ðŸ“š Concordance Examples for Top {KWIC_COLLOC_DISPLAY_LIMIT} MI Collocates (1 example per collocate)")
Â  Â  display_collocation_kwic_examples(
Â  Â  Â  Â  df_corpus=df,Â 
Â  Â  Â  Â  node_word=primary_target_mwu,Â 
Â  Â  Â  Â  top_collocates_df=full_mi,Â 
Â  Â  Â  Â  window=coll_window,
Â  Â  Â  Â  limit_per_collocate=1,
Â  Â  Â  Â  is_parallel_mode=is_parallel_mode,
Â  Â  Â  Â  target_sent_map=st.session_state['target_sent_map'],
Â  Â  Â  Â  show_pos=show_pos_tag,
Â  Â  Â  Â  show_lemma=show_lemma
Â  Â  )


st.caption("Tip: This app handles pre-tagged, raw, and now **Excel-based parallel corpora**.")
